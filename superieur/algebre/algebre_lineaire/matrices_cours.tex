\documentclass{book}
\usepackage{commeunjeustyle}

\begin{document}
\chapter*{Matrices}
\begin{Texte}
Les matrices sont des tableaux des éléments (nombres, caractères) qui servent à interpréter en termes calculatoires, et donc opérationnels, les résultats théoriques de l'algèbre linéaire et même de l'algèbre bilinéaire. Toutes les disciplines étudiant des phénomènes linéaires utilisent les matrices.\\ 
Notations : $\K $ désigne le corps $\R$ ou $\C$ et les lettres $n$,$p,\dots$ désignent des entiers naturels non nuls. Le mot \defi{scalaire} désigne un réel ou un complexe. 
\end{Texte}
\section{Exemples introductifs de modélisation matricielle}
\begin{Exemple}[Système d'équations linéaires]
La somme des tailles d'un fils et du père est de 2,5 mètres. La différence de tailles  est de 0.5 mètres.
Quel est la taille du fils ? \\
Comme vu au chapitre précédent, la première étape est la modélisation avec un système d'équations linéaires.\\
Soit la variable $x$ représentant la taille du fils et la variable $y$ représentant la taille du père.\\
Le couple $(x,y)$ vérifient le système suivant  :
$$(S)\quad \begin{cases}
x+y&=2,5\\
-x+y&=0,5
\end{cases}
$$
On sait résoudre par de simples opérations algébriques une équation linéaire à une inconnue à coefficient \impo{réels}. Par exemple, pour l'équation $2x=4$, on multiple par l'inverse de 2, ce qui donne $x=2$.\\
Une idée est de transformer le système de deux équations à deux inconnues à une équation à une inconnue. Cette transformation est une abstraction sur la nature des coefficients et des variables des équations : de \impo{réels} à  \impo{tableaux de nombres}.\\
Ainsi, l'équation équivalente au système d'équations linéaires est  :
\begin{center}
\begin{tikzpicture}[>=latex]
\matrix (A) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter = )] at (-2.5cm,0)
{%
  1 & 1   \\
 -1 & 1  \\
};
\matrix (B) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter =)] at (0,2.5cm)
{%
  x\\
  y\\
};
\draw[dotted] (0,0) node {$=$};
\matrix (Y) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter = )] at (2cm,0)
{%
  2,5  \\
  0,5 \\
};
\draw[<->,colordef](A-1-1) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (x) {$1\times x$} (B-1-1);
\draw[<->,colordef](A-1-2) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (y) {$1\times y$} (B-2-1);
\draw[colordef,->] (x) to node[midway,sloped,fill=white] {$+$} (y);
\end{tikzpicture}
\end{center}
Autrement dit, on $$A\times X =B$$ avec $A=\begin{pmatrix}
 1 & 1   \\
 -1 & 1  \\
\end{pmatrix}, X=\begin{pmatrix}
 x   \\
 y  \\
\end{pmatrix}$ et $B=\begin{pmatrix}
 2,5   \\
 0,5  \\
\end{pmatrix}$.\\
La matrice  $\begin{pmatrix}
 1 & 1   \\
 -1 & 1  \\
\end{pmatrix}$ est inversible, c'est à dire qu'il existe une matrice $A^{-1}=\begin{pmatrix}
 \frac 1 2 & -\frac 1 2   \\
 \frac 1 2 & \frac 1 2  \\
\end{pmatrix}$  telle que, en multipliant, par $A^{-1}$ les deux membres de l'équation $A\times X =B$, on obtient :
$$ X=A^{-1}\times B$$
$$ \begin{pmatrix}
 x   \\
 y  \\
\end{pmatrix}=\begin{pmatrix}
 \frac 1 2 & -\frac 1 2   \\
 \frac 1 2 & \frac 1 2  \\
\end{pmatrix} \begin{pmatrix}
 2,5   \\
 0,5  \\
\end{pmatrix}= \begin{pmatrix}
 1  \\
 1,5  \\
\end{pmatrix}.$$ 
Par identification, l'ensemble des solution est de nouveau l'unique solution $x=1$ et $y=1,5$. 
\end{Exemple}
\begin{Exemple}[Fonction linéaire]
Une usine fabrique des vélos, des trottinettes et des rollers. Le prix payé, $y$,   pour $x_1$ vélos, $x_2$ trottinettes et $x_3$ rollers est une fonction linéaires des prix unitaires d'achat 150 euros, 40 euros et 20 euros respectivement. On a donc :
$$y=150x_1+40x_2+20x_3.$$
La représentation matricielle de cette opération est :
\begin{center}
\begin{tikzpicture}[>=latex]
\draw[dotted] (-3,0) node {$y=$};
\matrix (A) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter = )] at (0,0)
{
  150 & 40 & 20  \\
};
\matrix (B) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter =)] at (3cm,3cm)
{
  x_1\\
  x_2\\
  x_3 \\
};
\draw[<->,colordef](A-1-1) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (x) {$150\times x_1$} (B-1-1);
\draw[<->,colordef](A-1-2) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (y) {$40\times x_2$} (B-2-1);
\draw[<->,colordef](A-1-3) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (z) {$20\times x_3$} (B-3-1);
\draw[colordef,->] (x) to node[midway,sloped,fill=white] {$+$} (y)%
                  to node[midway,sloped,fill=white] {$+$} (z);
\end{tikzpicture}
\end{center}
Les prix unitaires d'envoi sont respectivement égaux à 5$\EUR{5}$ pour un vélo, 2 euros pour la trottinette et 1 euros pour les rollers.   Le prix payé est encore une fonction linéaire :
$$Y=\begin{pmatrix}y_1\\y_2\end{pmatrix}=\begin{pmatrix} 150x_1+40x_2+20x_3 \\5x_1+2x_2+1x_3  \end{pmatrix}.$$
avec $y_1$ prix des marchandises et $y_2$ prix d'envoi.\\
La représentation avec un tableau de nombres (matricielle) de cette opération est :
\begin{center}
\begin{tikzpicture}[>=latex]
\matrix (Y) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter = )] at (-3cm,0)
{
  y_1  \\
  y_2 \\
};
\draw[dotted] (-2,0) node {$=$};
\matrix (A) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter = )] at (0,0)
{
  120 & 50 & 25  \\
  5   & 2 & 1 \\
};
\matrix (B) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter =)] at (3cm,3cm)
{
  x_1\\
  x_2\\
  x_3 \\
};
\draw[<->,colordef](A-2-1) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (x) {$5\times x_1$} (B-1-1);
\draw[<->,colordef](A-2-2) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (y) {$2\times x_2$} (B-2-1);
\draw[<->,colordef](A-2-3) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (z) {$1\times x_3$} (B-3-1);
\draw[colordef,->] (x) to node[midway,sloped,fill=white] {$+$} (y)%
                  to node[midway,sloped,fill=white] {$+$} (z);
\end{tikzpicture}
\end{center}
\end{Exemple}
%TODO faire les transformations du plan rotation, symétrie, shrink etc
%\subsection{Fonction linéaire}

\section{Les matrices}
\subsection{Définition}
\begin{Definition}[Matrice]
Une \defi{matrice}, $A$,  de taille $n\times p$  à coefficients  $\K $ est une famille d'élément de $\K $, $(a_{ij})_{\substack{1\leq i\leq n\\1\leq j\leq p}}$, présentée sous la forme d'un tableau :
$$ A=\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1p}\\
a_{21} & a_{22} & \cdots & a_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{np}\\
\end{pmatrix}.$$
Le scalaire $a_{ij}$ est appelé \defi{coefficient de A de position (i,j)}, noté aussi $(A)_{ij}$.\\
La matrice $C_j=\begin{pmatrix}
a_{1j}\\
\vdots\\
a_{nj}
\end{pmatrix}$ est appelée la \defi{$j^{ième}$ colonne} de $A$. On note $A=\begin{pmatrix}
&  & \\
C_1 & \cdots & C_{p}\\
&  & \\
\end{pmatrix}.$\\
La matrice $L_i=\begin{pmatrix}
a_{i,1}&\cdots&a_{i,p}
\end{pmatrix}$ est appelée la \defi{$i^{ième}$ ligne} de $A$. On note $A=\begin{pmatrix}
& L_1 & \\
 & \vdots & \\
&  L_n & \\
\end{pmatrix}.$\\ 
L'ensemble des matrices de taille $n\times p$ à coefficients dans $\K$ est noté \defi{$\MnpK$}.
\end{Definition}
\begin{Exemple}
Soit $A=\begin{pmatrix}1&2&3 \\4&5&6\\ \end{pmatrix}$. On a :
\begin{center}
$a_{11} = 1, a_{12} = 2, a_{13} = 3$\\
$a_{21} = 4, a_{22} = 5, a_{23} = 6$\\
$L_1=\begin{pmatrix}1&2&3 \end{pmatrix}, L_2=\begin{pmatrix}4&5&6\\ \end{pmatrix}$\\
$C_1=\begin{pmatrix}1\\4 \end{pmatrix}, C_2=\begin{pmatrix}2\\5 \end{pmatrix} , C_1=\begin{pmatrix}3\\6 \end{pmatrix}$
\end{center}
\end{Exemple}
\subsection{Matrices particulières}

\begin{Definition}[Matrices colonnes]
Une matrice à une colonne $\begin{pmatrix}a_1\\a_2\\ \vdots\\a_n \end{pmatrix}$ est appelée \defi{matrice colonne}.\\
L'ensemble des matrices colonnes de taille $n$ est noté \defi{$\M{n}{1}{\K}$} ou  \defi{$\K^n$} par identification avec l'ensemble des $n$-uplets.
\end{Definition}
\begin{Exemple}
Soit $X=\begin{pmatrix}x_1\\x_2\\\end{pmatrix}\in\K^2.$
\end{Exemple}

\begin{Definition}[Matrices lignes]
Une matrice à une ligne  $\begin{pmatrix}a_1&a_2& \dots &a_p \end{pmatrix}$ est appelée \defi{matrice ligne}.\\ 
L'ensemble des matrices ligne de taille $p$ est noté  \defi{$\M{1}{p}{\K}$} ou  \defi{$\K^p$} par identification avec l'ensemble des $p$-uplets.
\end{Definition}
\begin{Exemple}
Soit $Y=\begin{pmatrix}y_1&y_2&y_3\end{pmatrix}\in\K^3.$
\end{Exemple}

\begin{Definition}[Matrice nulle]
Une matrice ayant tous les coefficients nuls  est appelée \defi{matrice nulle}.\\
On la note $0_{n p}$ si elle a $n$ lignes et $p$ colonnes, ou $0$ s'il n'y a pas d'ambigüité.
\end{Definition}
\begin{Exemple}
$0_{32}=\begin{pmatrix}
0&0&0 \\0&0&0
\end{pmatrix}$.
\end{Exemple}

\begin{Definition}[Matrices carrés]
Une matrice ayant le même nombre de lignes et de colonnes $\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}\\
\end{pmatrix}$ est appelée \defi{matrice carré}.\\
La famille $\begin{pmatrix}
a_{11}&a_{22}&\cdots&a_{nn}
\end{pmatrix}$ est appelé \defi{diagonale de A}.\\ 
L'ensemble des matrices carrés de taille $n$ est noté \defi{$\MnK$}.\\
\end{Definition}
\begin{Exemple}
La matrice $J= \begin{pmatrix}1&1\\1&1\end{pmatrix}$ est carrée.
\end{Exemple}
\begin{Definition}[Matrices triangulaires supérieurs]
Une matrice carrée ayant tous les coefficients strictement au dessous de la
diagonale sont nuls 
$$\begin{pmatrix}a_{1,1}&a_{1,2}&\cdots &\cdots &a_{1,n}\\0&a_{2,2}&&&a_{2,n}\\\vdots &\ddots &\ddots &&\vdots \\\vdots &&\ddots &\ddots &\vdots \\0&\cdots &\cdots &0&a_{n,n}\\\end{pmatrix}$$ est appelée \defi{matrice triangulaire supérieur}.
\end{Definition}
\begin{Exemple}
La matrice $\begin{pmatrix}1&2&3\\0&4&5\\0&0&6\end{pmatrix}$ est triangulaire supérieur.
\end{Exemple}
\begin{Definition}[Matrices triangulaires inférieurs]
Une matrice carrée ayant tous les coefficients strictement au dessus de la
diagonale sont nuls 
$$\begin{pmatrix}a_{1,1}&0&\cdots &\cdots &0\\a_{2,1}&a_{2,2}&\ddots &&\vdots \\\vdots &&\ddots &\ddots &\vdots \\\vdots &&&\ddots &0\\a_{n,1}&a_{n,2}&\cdots &\cdots &a_{n,n}\\\end{pmatrix}$$ est appelée \defi{matrice triangulaire inférieur}.
\end{Definition}
\begin{Definition}[Matrices diagonales]
Une matrice carrée étant  à la fois triangulaires supérieures et triangulaires
inférieures, c'est à dire les seuls coefficients pouvant être non nuls sont donc ceux de la diagonale
$$
\begin{pmatrix}a _{11}&0&\dots &0\\0&a _{22}&\dots &0\\\vdots &\vdots &\ddots &\vdots \\0&0&\dots &a _{nn}\end{pmatrix}
$$
 est appelée \defi{matrice diagonale}.
\end{Definition}
\begin{Exemple}
La matrice $\begin{pmatrix}1&0&0\\0&-1&0\\0&0&0\end{pmatrix}$ est diagonale.
\end{Exemple}
\begin{Definition}[Matrice identité]
La \defi{matrice identité}, noté \defi{$I_n$}, est une matrice diagonale dont les coefficients les diagonaux sont égaux à 1: $$I_n=\begin{pmatrix}1&0&\cdots &0\\0&\ddots &\ddots &\vdots \\\vdots &\ddots &\ddots &0\\0&\cdots &0&1\end{pmatrix}.$$
\end{Definition}


\section{Calcul matriciel}
\subsection{Égalité}
\begin{Definition}[Égalité des matrices]
Deux matrices $A$ et $B$ sont égales, ce qu'on note $A = B$ si
\begin{itemize}
\item même nombre de lignes,
\item même nombre de colonnes,
\item les coefficients à la même position sont égaux.
\end{itemize}
\end{Definition}
\begin{Exemple}
$$\begin{pmatrix}1^2&2^2\\3^2&4^2\end{pmatrix}=\begin{pmatrix}1&4\\9&16\end{pmatrix}$$
et
$$\begin{pmatrix}0&0\\0&0\\0&0\\\end{pmatrix}\neq\begin{pmatrix}0&0\\0&0\\\end{pmatrix}$$
\end{Exemple}
\subsection{Addition matricielle}
\begin{Definition}[Somme]
Soit $A,B\in \MnpK$ ayant  même nombre de lignes $n$ et
même nombre de colonnes $p$.\\
La \defi{somme $A+B$} de $A$ et $B$ est égale la matrice dont chaque coefficient est la somme des coefficients de même position de $A$ et
de $B$, c'est à dire : 
$$\begin{pmatrix}
a_{11} & \cdots & a_{1p}\\
 \vdots &  & \vdots\\
a_{n1} & \cdots & a_{np}
\end{pmatrix}+\begin{pmatrix}
b_{11} & \cdots & b_{1p}\\
 \vdots &  & \vdots\\
b_{n1} & \cdots & b_{np}
\end{pmatrix}= \begin{pmatrix}
a_{11}+b_{11} & \cdots & a_{1p}+b_{1p}\\
 \vdots &  & \vdots\\
a_{n1}+b_{n1} & \cdots & a_{np}+b_{np}
\end{pmatrix}.$$
\end{Definition}
\begin{Exemple}$
\begin{pmatrix}
0 &1 & 2 \\
3 &4 & 5 \\
\end{pmatrix} + 
\begin{pmatrix}
-2 &-1 & 1 \\
0 & 1 & 0 \\
\end{pmatrix}
=\begin{pmatrix}
0-2 &1-1 & 2+1\\
3+0 & 4+1 & 5+0\\
\end{pmatrix}=
\begin{pmatrix}
-2 &0 & 3\\
3 & 5 & 5\\
\end{pmatrix}
$
\end{Exemple}

\begin{Proposition}[Structure de groupe commutatif]
Soit $A,B,C\in \MnpK.$
\begin{itemize}
\item \propri{Commutative} : $A+B=B+A$
\item \propri{Associative} : $(A+B)+C=A+(B+C)$
\item \propri{Élément neutre $0_{np}$} : $A+0_{np}=0_{np}+A=A$
\item \propri{Symétrique} : en notant $(-A)=(-a_{ij})_{\substack{1\leq i\leq n\\1\leq j\leq p}}$, on a :
$$A+(-A)=0_{np}.$$
\end{itemize}
\end{Proposition}
\begin{Demonstration}
\begin{itemize}
\item Commutative : $$\begin{aligned}
A+B=&\begin{pmatrix}
a_{11} & \cdots & a_{1p}\\
 \vdots &  & \vdots\\
a_{n1} & \cdots & a_{np}
\end{pmatrix}+\begin{pmatrix}
b_{11} & \cdots & b_{1p}\\
 \vdots &  & \vdots\\
b_{n1} & \cdots & b_{np}
\end{pmatrix}\overbrace{=}^{\text{Déf Addition matricielle}}\begin{pmatrix}
a_{11}+b_{11} & \cdots & a_{1p}+b_{1p}\\
 \vdots &  & \vdots\\
a_{n1}+b_{n1} & \cdots & a_{np}+b_{np}
\end{pmatrix}\\
A+B=&\begin{pmatrix}
b_{11}+ a_{11} & \cdots & b_{1p}+a_{1p}\\
 \vdots &  & \vdots\\
b_{n1}+a_{n1} & \cdots & b_{np}+a_{np}
\end{pmatrix}=\begin{pmatrix}
b_{11} & \cdots & b_{1p}\\
 \vdots &  & \vdots\\
b_{n1} & \cdots & b_{np}
\end{pmatrix}+\begin{pmatrix}
a_{11} & \cdots & a_{1p}\\
 \vdots &  & \vdots\\
a_{n1} & \cdots & a_{np}
\end{pmatrix}\\
A+B=&B+A.
\end{aligned}$$
\item Associative : démonstration similaire à la commutativité
\item Élément neutre : à droite
$$\begin{pmatrix}
a_{11} & \cdots & a_{1p}\\
 \vdots &  & \vdots\\
a_{n1} & \cdots & a_{np}
\end{pmatrix}+\begin{pmatrix}
0 & \cdots & 0\\
 \vdots &  & \vdots\\
0 & \cdots & 0
\end{pmatrix}=\begin{pmatrix}
a_{11}+0 & \cdots & a_{1p}+0\\
 \vdots &  & \vdots\\
a_{n1}+0 & \cdots & a_{np}+0
\end{pmatrix}=\begin{pmatrix}
a_{11} & \cdots & a_{1p}\\
 \vdots &  & \vdots\\
a_{n1} & \cdots & a_{np}
\end{pmatrix}
$$
On ferait de même à gauche. 
\item Symétrique :
$$\begin{pmatrix}
a_{11} & \cdots & a_{1p}\\
 \vdots &  & \vdots\\
a_{n1} & \cdots & a_{np}
\end{pmatrix}+\begin{pmatrix}
-a_{11} & \cdots & -a_{1p}\\
 \vdots &  & \vdots\\
-a_{n1} & \cdots & -a_{np}
\end{pmatrix}=\begin{pmatrix}
a_{11}+a_{11} & \cdots & a_{1p}-a_{1p}\\
 \vdots &  & \vdots\\
a_{n1}-a_{n1}& \cdots & a_{np}-a_{np}
\end{pmatrix}=\begin{pmatrix}
0 & \cdots & 0\\
 \vdots &  & \vdots\\
0 & \cdots & 0
\end{pmatrix}
$$
\end{itemize}
\end{Demonstration}
\subsection{Multiplication d'une matrice par un élément de \(\K\)}
\begin{Definition}[Produit externe]
Soit $A\in \MnpK$ et $\lambda\in\K$.\\
La \defi{produit $\lambda A$} de $\lambda$ par $A$ est égale la matrice dont chaque coefficient est obtenu en multipliant le coefficient
de même position de $A$ par $\lambda$ :
$$\lambda \begin{pmatrix}
a_{11} & \cdots & a_{1p}\\
 \vdots &  & \vdots\\
a_{n1} & \cdots & a_{np}
\end{pmatrix}= \begin{pmatrix}
\lambda a_{11} & \cdots & \lambda a_{1p}\\
 \vdots &  & \vdots\\
\lambda a_{n1} & \cdots & \lambda a_{np}
\end{pmatrix}.$$
\end{Definition}
\begin{Exemple}
 $
2\begin{pmatrix}
0 &1 & 2 \\
3 &4 & 5 \\
\end{pmatrix} 
=\begin{pmatrix}
2\times 0 &2\times 1 & 2\times 2 \\
2\times 3 &2\times 4 & 2\times 5 \\
\end{pmatrix} =
\begin{pmatrix}
0 &2 & 4 \\
6 &8 & 10 \\
\end{pmatrix}
$
\end{Exemple}
\begin{Proposition}[Propriétés]
 Soit $\lambda,\mu\in\K$ et $A,B \in \MnpK$.\\
Alors,
\begin{itemize}
\item $\lambda(A + B) = \lambda A + \lambda B$
\item $(\lambda+ \mu)A = \lambda A + \mu A$
\item $(\lambda \mu)A = \lambda(\mu A)$
\item $1A=A$
\end{itemize}
\end{Proposition}
\begin{Demonstration}
La démonstration est évidente. 
\end{Demonstration}
\subsection{Multiplication matricielle}
\begin{Definition}[Produit matriciel]
Soit $A\in\MnpK$ et $B\in\M{p}{q}{\K}$.\\
Le \defi{produit matriciel  $A \times B$} est la matrice $C=\M{n}{q}{\K}$ définie par :
$$\forall i\in \Intf{1}{n},\forall j\in \Intf{1}{q} : \quad  c_{ij}=\sum_{k=1}^p a_{ik}b_{kj}.$$
\begin{tikzpicture}[>=latex]

\matrix (A) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter = )] at (0,0)
{
  a_{11} & a_{12} & \ldots & a_{1p}  \\
  a_{21}%
         &  a_{22}%
                  & \ldots%
                           & a_{2p} \\
  \vdots & \vdots & \ddots & \vdots  \\
  a_{n1} & a_{n2} & \ldots & a_{np}  \\
};
\node [draw,below=10pt] at (A.south) 
    { $A$ : $n$ lignes $p$ colonnes};

\matrix (B) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter =)] at (6cm,6cm)
{
  b_{11} &  b_{12}%
                  & \ldots & b_{1q}  \\
  b_{21} &  b_{22}%
                  & \ldots & b_{2q}  \\
  \vdots & \vdots & \ddots & \vdots  \\
  b_{p1} & b_{p2}%
                  & \ldots & b_{pq}  \\
};
\node [draw,above=10pt] at (B.north) 
    { $B$ : $p$ lignes $q$ colonnes};
\matrix (C) [matrix of math nodes,%
             nodes = {circle,minimum size=1cm},%
             left delimiter  = (,%
             right delimiter = )] at (6cm,0)
{
  c_{11} & c_{12} & \ldots & c_{1q} \\
  c_{21} &  c_{22}%
                  & \ldots & c_{2q} \\
  \vdots & \vdots & \ddots & \vdots \\
  c_{n1} & c_{n2} & \ldots & c_{nq} \\
};
% les fleches
%\draw[colorprop] (A-2-1.north) -- (C-2-2.north);
%\draw[colorprop] (A-2-1.south) -- (C-2-2.south);
%\draw[colorprop] (B-1-2.west)  -- (C-2-2.west);
%\draw[colorprop] (B-1-2.east)  -- (C-2-2.east);
\draw[<->,colordef](A-2-1) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (x) {$a_{21}\times b_{12}$} (B-1-2);
\draw[<->,colordef](A-2-2) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (y) {$a_{22}\times b_{22}$} (B-2-2);
\draw[<->,colordef](A-2-4) to[in=180,out=90]
    node[draw,sloped,midway,fill=white] (z) {$a_{2p}\times b_{p2}$} (B-4-2);
\draw[colordef,->] (x) to node[midway,sloped,fill=white] {$+$} (y)%
                  to node[midway,sloped,fill=white] {$+\raisebox{.5ex}{\ldots}+$} (z)%
                  to (C-2-2.north west);
\node [draw,below=10pt] at (C.south) 
    {$C= A\times B$ : $n$ lignes  $q$ colonnes};
    mm
\end{tikzpicture}\\


On omettra souvent symbole $\times$ dans les expressions : \defi{$AB =A\times B$}.
\end{Definition}
\begin{Exemple}
$\begin{pmatrix}
\color{orange}2 &\color{orange}-1 & \color{orange}2 \\
\color{yellow}1 &\color{yellow}0 & \color{yellow}3 \\
\end{pmatrix}\begin{pmatrix}
\color{green}1 & \color{pink}2  \\
\color{green}3 &\color{pink}4 \\
\color{green}5&\color{pink}6\\
\end{pmatrix}=\begin{pmatrix}
\color{orange}2\color{black}\times \color{green} 1\color{black}+\color{orange}(-1)\color{black}\times \color{green} 3 \color{black}+\color{orange}2\color{black}\times \color{green} 5&
\color{orange}2\color{black}\times \color{pink}2\color{black}+\color{orange}(-1)\color{black}\times \color{pink}4 \color{black}+\color{orange}2\color{black}\times \color{pink} 6  \\
\color{yellow}1\color{black}\times \color{green} 1\color{black}+\color{yellow}0\color{black}\times \color{green} 3 \color{black}+\color{yellow}3\color{black}\times \color{green} 5&
\color{yellow}1\color{black}\times \color{pink}2\color{black}+\color{yellow}0\color{black}\times \color{pink}4 \color{black}+\color{yellow}3\color{black}\times \color{pink} 6  \\
\end{pmatrix}=\begin{pmatrix}
11 &12\\
16 &20\\
\end{pmatrix}$
\end{Exemple}


\begin{Remarque}[Algèbre non commutative et non intègre]En générale, le produit matriciel n'est pas commutatif ($AB = BA$). Par exemple, si $A=\begin{pmatrix}0&1\\0&0\\\end{pmatrix}$ et $B=\begin{pmatrix}1&0\\0&0\\\end{pmatrix}$. On a $A\times B=\begin{pmatrix}0&0\\0&0\\\end{pmatrix}$ tandis que 
$B\times A = \begin{pmatrix}0&1\\0&0\\\end{pmatrix}$.\\
De plus, en générale, le produit matriciel n'est pas intègre (si $AB =0$ alors $A=0$ ou $B=0$). C'est à dire un produit de matrices peut être nul sans qu'aucune d'entre elles le soit. Par exemple, $$\begin{pmatrix}0&1\\0&0\\\end{pmatrix}\times \begin{pmatrix}1&0\\0&0\\\end{pmatrix}=\begin{pmatrix}0&0\\0&0\\\end{pmatrix}.$$
\end{Remarque}

\begin{Proposition}[Lignes et colonnes d'un produit matriciel]
On a : 
$$A\times\begin{pmatrix}0\\\vdots\\0\\1\text{}\\0\\\vdots\\0\end{pmatrix}=\begin{pmatrix}
&  & \\
C_1 & \cdots&C_j&\cdots & C_{p}\\
&  & \\\end{pmatrix} \begin{pmatrix}0\\\vdots\\0\\1\\0\\\vdots\\0\end{pmatrix}=C_j$$
et
$$\begin{pmatrix}0&\dots&0& 1&0&\ldots&0\end{pmatrix} \times A= \begin{pmatrix}0&\dots&0& 1&0&\ldots&0\end{pmatrix} \times \begin{pmatrix}L_1&\dots& L_i&\ldots&L_n\end{pmatrix}=L_i.$$
Et plus généralement :
$$A\times\begin{pmatrix}x_1\\\vdots\\x_p\end{pmatrix}=\overbrace{\color{black}\sum_{j=1}^p x_j C_j}^{\color{colorprop}\substack{\text{Combinaison linéaire}\\\text{des vecteurs }C_1,\dots,C_p}}$$
et
$$\begin{pmatrix}x_1&\dots&x_n\end{pmatrix}\times A=\sum_{i=1}^n x_i L_i$$

\end{Proposition}

%\begin{Exemple}[Exemple introductif]
%Une entreprise fabrique deux types de téléviseurs, téléviseurs HD et téléviseurs 4k. Pour fabriquer,
%\begin{itemize}
%\item  un téléviseur HD il faut 1 unité de bureau d'études, 2 unités de main d'\oe uvre et 3 unités de composants électroniques,
%\item un téléviseurs 4k il faut 2 unités de bureau d'études, 3 unités de main d'\oe uvre et 6 unités de composants électroniques.
%\end{itemize}
%Le coût des unités sont les suivants : l'unité de bureau d'études coût 45 euros ; l'unité de main d'\oe uvre coûte 25 euros ; l'unité de composants coûte 30 euros.\\
%L'entreprise doit fabriquer 100 téléviseurs HD et 40 téléviseurs 4K.\\
%Déterminons le coût total de cette commande.\\
%On considère les trois matrices :
%\begin{itemize}
%\item Matrice A des coûts respectifs d'une unité de bureau d'études, de main d'\oe uvre et de composants électronique. La matrice A est une matrice 1 ligne, 3 colonnes :
%$$A=\begin{pmatrix}
%45& 25 & 30\end{pmatrix}$$
%\item Matrice B des quantités, sachant que la première colonne donne les quantités nécessaires en unités de bureau d'étude, de main d'\oe uvre et de composants électroniques pour un téléviseur HD et la seconde colonne les quantités nécessaires en unités de bureau d'études, de main d'\oe uvre et de composants pour la fabrication d'un téléviseur 4K. La matrice B est donc une matrice 3 lignes et 2 colonnes :
%$$B=\begin{pmatrix}
%1& 2 \\
%2 &3 \\
%3& 6
%\end{pmatrix}$$
%\item Matrice C qui donne le nombre de téléviseurs HD et le nombre de téléviseurs 4K. La matrice C est une matrice 2 lignes, 1 colonne.
%$$B=\begin{pmatrix}
%100 \\
%40
%\end{pmatrix}$$
%\end{itemize}
%$A\times B$ est la matrice qui donne le coût pour la fabrication d'un téléviseur HD et d'un téléviseur 4K.
%$$ A\times B = \begin{pmatrix}
%45& 25 & 30\end{pmatrix}\times \begin{pmatrix}
%1& 2 \\
%2 &3 \\
%3& 6
%\end{pmatrix} =  \begin{pmatrix}
%45\times 1 + 25\times 2 + 30 \times 3   & 45\times 2 + 25\times 3 + 30 \times 6\end{pmatrix}=\begin{pmatrix} 185 & 345\end{pmatrix}  
%$$
%Le coût total de la fabrication est donc la matrice :
%$$(A\times B)\times C= \begin{pmatrix} 185 & 345\end{pmatrix} \times  \begin{pmatrix} 100 \\ 40\end{pmatrix} =\begin{pmatrix} 185\times 100+ 345\times40\end{pmatrix}= \begin{pmatrix} 323000\end{pmatrix}.$$
%\end{Exemple}
\begin{Exemple}
La chaine  youtube 3Blue1Brown donne un point de vue géométrique du calcul matriciel (voir \url{https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=1}) 
\end{Exemple}

\begin{Proposition}[Propriétés]
\begin{enumerate}
\item
  \propri{Associative} : $$ (A  B) C = A  (B C).$$
\item
  \propri{Distributivité par rapport à l'addition} :  $$  ( A + B) C = A C +  B C\quad\text{ et }\quad A (B+ C) = AB + AC.$$
\item
  \propri{Élément neutre} : la matrice identité est l'élément neutre, c'est à dire  
 $$\forall A \in   \M{n}{q}{\K} :\quad I_n A =A I_q = A.$$
\end{enumerate}
\end{Proposition}
\begin{Demonstration}
\begin{enumerate}
\item \textit{Associative} : Soit $i,j\in\Intf{1}{n}$. On a :
$$
\begin{aligned}
\left((A \times B) \times C\right)_{ij}=&\sum_{k=1}^q (A \times B)_{ik}\,c_{kj}=\sum_{k=1}^q \left(\sum_{l=1}^p a_{il}b_{lk}\right) c_{kj}=\sum_{l=1}^p  a_{il}\left(\sum_{k=1}^q b_{lk} c_{kj}\right)\\
=&\sum_{l=1}^p a_{il}(B \times C)_{lj}=\left(A \times (B \times C)\right)_{ij}
\end{aligned}$$
\item Les deux autres propriétés se démontrent aisément. 
\end{enumerate}
\end{Demonstration}
\subsection{Puissance d'une matrice carrée}
\begin{Definition}[Puissance]
La \defi{puissance} d'une matrice $A\in \MnK$ est la multiplication itérée de  $A$, soit
$$\forall k \in \N^*:\quad  A^k = \overbrace{A\times A\times \dots\times A}^{\text{k fois}} \quad \text{et}\quad A^0=I_n.$$
\end{Definition}
\begin{Exemple}[Puissance d'une matrice de 1]
Déterminer la puissance  $J =\begin{pmatrix}
1&\dots&1\\
\vdots& &\vdots\\
1 &\dots &1\\
\end{pmatrix}$.
\begin{Demonstration}
On a
$J^2 = nJ$ puis par récurrence, pour $k \geq 1$, $J^k = n^{k-1}J.$
\end{Demonstration}
\end{Exemple}
\begin{Proposition}[Propriétés]
Soit $A\in\MnK$, $\lambda\in\R$ et $k,l\in\N$.\\
On a :
\begin{itemize}
\item $A^k A^l=A^{k+l}$
\item $\left(A^k\right)^l=A^{kl}$
\item $\left(\lambda A\right)^k=\lambda^k A^{k}$
\end{itemize}
\end{Proposition}

Comme la multiplication n'est pas commutative, les identités binomiales usuelles sont fausses. En particulier, $(A+ B)^2$ ne vaut en général pas $A^2 + 2AB + B^2$
 mais  seulement $A^2 +  \color{colordef}AB + BA\color{black}+ B^2$.
\begin{Proposition}[Formule du binôme]
Soit $A,B\in\MnK$  qui commutent, c'est à dire $\color{colorprop}AB = BA$.\\
Alors, pour tout entier $p > 0,$ on a les formules
$$(A+ B)^p=\sum_{k=0}^p\begin{pmatrix}
p\\k
\end{pmatrix} A^{k}B^{p-k}$$
et 
$$ A^p-B^p=(A-B)\sum_{k=0}^{p-1}A^k B^{p-1-k} $$
où $\begin{pmatrix}
p\\n
\end{pmatrix}$ désigne le coefficient du binôme.
\end{Proposition}
\begin{Demonstration}
La démonstration par récurrence est similaire à celle de la formule du binôme pour $(a + b)^p$, avec $a, b$ des réels. 
\end{Demonstration}

\begin{Definition}[Matrice nilpotente]
On dit que A est \defi{nilpotente} si il existe  $k\in \N^* : A^k = 0.$\\
Le plus petit entier $k$ pour laquelle cette identité est vraie est appelé l'\defi{indice de nilpotence} de $A$.
\end{Definition}
\begin{Exemple}
Déterminer la puissance de $A=\begin{pmatrix}
1&2&0\\
0& 1&2\\
0&0 &1\\
\end{pmatrix}$.
\begin{Demonstration}
On a $A=I_3+2N$ avec $N=\begin{pmatrix}
0&1&0\\
0& 0&1\\
0&0 &0\\
\end{pmatrix}$.\\
On a $$N^3=\begin{pmatrix}
0&1&0\\
0& 0&1\\
0&0 &0\\
\end{pmatrix}\begin{pmatrix}
0&1&0\\
0& 0&1\\
0&0 &0\\
\end{pmatrix}\begin{pmatrix}
0&1&0\\
0& 0&1\\
0&0 &0\\
\end{pmatrix} = \begin{pmatrix}
0&0&1\\
0& 0&0\\
0&0 &0\\
\end{pmatrix}\begin{pmatrix}
0&1&0\\
0& 0&1\\
0&0 &0\\
\end{pmatrix}=\begin{pmatrix}
0&0&0\\
0& 0&0\\
0&0 &0\\
\end{pmatrix}.$$
La matrice $N$ est donc nilpotente d'indice 2 et commute avec $I_3$. On a :
$$A^p= (2N +I_3 )^p=\sum_{k=0}^p\begin{pmatrix}
p\\k
\end{pmatrix} (2N)^{k}I_3^{p-k}=\begin{pmatrix}
p\\0
\end{pmatrix}(2N)^{0}+\begin{pmatrix}
p\\1
\end{pmatrix}(2N)^{1}+\overbrace{(2N)^{2}+\dots}^{=0}=I_3+2pN.$$
\end{Demonstration}
\end{Exemple}
\subsection{Transposée}
\begin{Definition}[Transposée]
La \defi{matrice transposée} (ou la \defi{transposée}) d'une matrice $A=(a_{ij})_{\substack{1\leq i\leq n\\1\leq j\leq p}}\in \MnpK$ est la matrice $\transposee{A}=(a_{ji})_{\substack{1\leq j\leq n\\1\leq i\leq p}}\in\M{p}{n}{\K}$ obtenue en échangeant les lignes et les colonnes de $A$.
\end{Definition}
\begin{Exemple}
Par exemple, si  $A={\begin{pmatrix}1&3&5\\2&4&6\end{pmatrix}}$
alors $\transposee{A}={\begin{pmatrix}1&2\\3&4\\5&6\end{pmatrix}}$.
\end{Exemple}
\begin{Proposition}[Propriétés]
\begin{itemize}
\item \propri{Linéaire} :  $\forall A, B \in \MnpK,\forall \lambda,\mu\in \K :\quad  \transposee{\left(\lambda A+ \mu B\right)}= \lambda\transposee{A} + \mu\transposee{B}$
\item \propri{Involutive} : $\forall A \in \MnpK: \quad \transposee{\left(\transposee{A}\right)}=A$
\item \propri{Ordre inversé produit} : $\forall A\in \MnpK,\forall b\in \M{p}{q}{K} :\quad  \transposee{\left(AB\right)}= \transposee{B} \transposee{A}$
\end{itemize}
\end{Proposition}
\begin{Demonstration}
\begin{itemize}
\item Linéaire : $$(\transposee{(\lambda A+ \mu B)})_{ij}\overbrace{=}^{\text{Def transposée}} (\lambda A+ \mu B)_{ji}=\lambda (A)_{ji}+\mu (B)_{ji}=\lambda (\transposee{A})_{ij}+\mu (\transposee{B})_{ij} $$
\item Involutive : $$(\transposee{\left(\transposee{A}\right)})_{ij}=(\transposee{A})_{ji}=(A)_{ij}$$
\item Ordre inversé produit : $$(\transposee{\left(AB \right)})_{ij}=(AB)_{ji}=\sum_{k=1}^p (A)_{jk}(B)_{ki}=\sum_{k=1}^p\sum_{k=1}^p (\transposee{B})_{ik}  (\transposee{A})_{kj}=( \transposee{B}\transposee{A})_{ij}.$$
\end{itemize}
\end{Demonstration}
\begin{Definition}[Matrice symétrique et antisymétrique]
 On dit que $A$ est \defi{symétrique} si : $\transposee{A} = A$.\\
 On dit que $A$ est \defi{antisymétrique} si : $\transposee{A} = -A$.
\end{Definition}
\begin{Exemple}
La matrice $\begin{pmatrix}2&4&6\\4&0&10\\6&10&12\end{pmatrix}$ est  symétrique.
\end{Exemple}

\subsection{Trace d'une matrice carrée}
\begin{Definition}[Trace]
La \defi{trace}  d'une matrice $A=\begin{pmatrix}
\mathbin{\color{colordef}a_{11}} & a_{12} & \cdots & a_{1n}\\
a_{21} & \mathbin{\color{colordef}a_{22}} & \cdots & a_{2n}\\
\vdots & \vdots & \mathbin{\color{colordef}\ddots} & \vdots\\
a_{n1} & a_{n2} & \cdots & \mathbin{\color{colordef}a_{nn}}\\
\end{pmatrix}\in \MnK$ est le nombre obtenu en additionnant les éléments diagonaux , soit
$$ \Tr(A) =\mathbin{\color{colordef}a_{11}+a_{22}+\dots+ a_{nn}}=\sum  _{k=1}^n a_{k k}. $$
\end{Definition}
\begin{Exemple}
On a $\Tr(\begin{pmatrix}-1&0&3\\11&5&2\\6&12&-5\end{pmatrix})=-1+5+(-5)=-1.$
\end{Exemple}
\begin{Exemple}[Norme sur $\MnpK$]
L'application  $A\mapsto \Tr(\transposee{A} A)=\sum_{i=1}^n\sum_{j=1}^p a_{ij}^2$ est une norme sur $\MnpK$ (voir chapitre espace pré-hilbertien).
\end{Exemple}
\begin{Proposition}[Propriétés]
\begin{itemize}
\item \propri{Linéaire} :  $\forall A, B \in \MnpK,\forall \lambda,\mu\in \K :\quad  \Tr\left(\lambda A+ \mu B\right)= \lambda\Tr(A) + \mu\Tr(B)$
\item  \propri{Ordre inversé produit} : $\forall A\in \MnpK,\forall b\in \M{p}{b}{K} :\quad  \Tr(AB)= \Tr(BA)$
\end{itemize}
\end{Proposition}
\begin{Demonstration} On a :
\begin{itemize}
\item $$\Tr\left(\lambda A+ \mu B\right)=\sum  _{k=1}^n (\lambda a_{k k}+\mu b_{kk})=\lambda \sum  _{k=1}^n a_{k k}+\mu \sum  _{k=1}^n b_{kk}=\lambda \Tr (A)+\mu \Tr(B)$$
\item $$\Tr( AB)=\sum_{k=1}^n (AB)_{kk}=\sum _{k=1}^n\sum_{p=1}^n  a_{kp}b_{pk}=\sum_{p=1}^n \sum _{k=1}^n b_{pk}  a_{kp}=\sum_{p=1}^n(AB)_{pp}=\Tr(BA).  $$
\end{itemize}

\end{Demonstration}
\begin{Remarque}
En général, $\Tr(ABC)\neq \Tr(ACB)$.
\end{Remarque}


\section{Matrices inversibles}
On sait résoudre une équation $a x = b$ à une inconnue à coefficient \impo{réels}.   Cette équation  :
\begin{itemize}
\item si $a\neq 0$ : admet l'unique solution $a^{-1}b$ si $a\neq 0$,
\item si $a\neq 0$ 
\begin{itemize}
\item si $b\neq 0$ n'admet pas de solution ,
\item si $b=0$  admet l'ensemble $\R$ comme solutions.
\end{itemize}
\end{itemize}
Pour déterminer l'ensemble des solutions, on se pose la question de l'existence de l'inverse de $a$. Pour les matrices, résoudre $AX = B$ revient à se poser le même question sur  existence de l'inverse de $A$.
\subsection{Définition}
\begin{DefinitionProposition}[Inverse]
On dit qu'une matrice $A\in\MnK$ est inversible s'il existe $B\in\MnK$ tel que $$AB=BA=I_n.$$ 
Si elle existe, la matrice $B$ est  unique et appelée \defi{inverse} de $A$, noté \defi{$A^{-1}$}.\\
L'ensemble des matrice inversibles de taille $n$ se note $\GLnK$ et appelé \defi{groupe linéaire}.  
\end{DefinitionProposition}
\begin{Demonstration}
Pour l'unicité, on suppose l'existence de deux matrices $B$ et $B'$ inverse de $A$. On a :
$$ B = BI_n =B(AB')=(BA)B'=I_n B'=B'.$$
Ainsi $B=B'$.
\end{Demonstration}
\begin{Exemple}[Matrice identité]
L'inverse de la matrice identité est elle-même car $I_nI_n=I_n$.
\end{Exemple}
\begin{Exemple}[Matrice nulle]
La matrice nulle $0_n$ n'est pas inversible car  pour toute matrice $A\in\MnK$, on a
$A 0_n = 0_n$ donc le produit d'une matrice avec la matrice $0_n$ n'est jamais égale à la matrice identité.
\end{Exemple}



\subsection{Propriétés}
\begin{Proposition}[A gauche ou à droite]
Il suffit de vérifier que $AB=I_{n}$ ou bien $BA=I_n$ pour prouver que $A$ est inversible d'inverse $B$.  
\end{Proposition}
\begin{Demonstration}
Voir chapitre sur les applications linéaires. 
\end{Demonstration}
\begin{Proposition}[Opérations sur les matrices inversibles]
Soit $A$ et $B$ deux matrices \impo{inversibles}.\\
\begin{itemize}
\item \propri{Inversibilité de l'inverse} : $A^{-1}$ est inversible et $\left(A^{-1}\right)^{-1}=A$.
\item \propri{Inversibilité du produit} : $AB$ est inversible et $\left(AB\right)^{-1}=B^{-1} A^{-1}$. 
\item \propri{Inversibilité de la transposée} : $\transposee{A}$ est inversible et $\left(\transposee{A}\right)^{-1}=\transposee{\left(A^{-1}\right)}$. 
\end{itemize} 
\end{Proposition}
\begin{Demonstration}
\begin{itemize}
\item Inversibilité de l'inverse : comme  $A A^{-1}=I_n$, l'inverse de $A^{-1}$ est $A$.
\item Inversibilité du produit : $\left(B^{-1} A^{-1}\right)\left(AB\right)\overbrace{=}^{\text{Associative}}B^{-1}(A^{-1}A)B=B^{-1}(I_n)B=B^{-1}B=I_n. $
\item Inversibilité de la transposée : $\transposee{\left(A^{-1}\right)}\transposee{A}=\transposee{\left(AA^{-1}\right)}=\transposee{I_n}=I_n $. 
\end{itemize} 
\end{Demonstration}

\begin{Proposition}[Simplification d'une équation]
Soit $C$ une matrice inversible.\\
Alors l'égalité $AC = BC$ implique l'égalité $A = B.$
\end{Proposition}
\begin{Demonstration}
On multiplie à droite par l'inverse de $C$. On a 
 $(AC)C^{-1} = (BC)C^{-1}$. Comme le produit matricielle est associative, on a  $A(CC^{-1}) = B(CC^{-1})$, d'où $A=B$.
\end{Demonstration}


\begin{Proposition}[Caractérisation de l'inversibilité 1]
Soit $A\in\MnK$.\\
Une colonne (ou une ligne) de $A$ est combinaison linéaire des autres colonnes (ou des autres colonnes) si et seulement si $A$ n'est pas inversible.
\end{Proposition}
\begin{Demonstration}
Soit $A=\begin{pmatrix}
&  & \\
C_1 & \cdots & C_{n}\\
&  & \\
\end{pmatrix}\in\MnK$.\\
\begin{itemize}
\item Sans perte de généralité, supposons que la première colonne est combinaison linéaire des autres, c'est à dire qu'il existe $\lambda_2,\dots,\lambda_n\in\K$ tel que :
$$C_1=\lambda_2 C_2+\dots+\lambda_n C_n.$$
Supposons par l'\impo{absurde} que $A$ est inversible.\\
On a :
$$A\begin{pmatrix}
1 \\
-\lambda_2\\
\vdots \\
-\lambda_n\\
\end{pmatrix}=\begin{pmatrix}
&&   & \\
C_1 &C_2& \cdots & C_{n}\\
& &  & \\
\end{pmatrix}\begin{pmatrix}
1 \\
-\lambda_2\\
\vdots \\
-\lambda_n\\
\end{pmatrix}=C_1-\lambda_2 C_2-\dots-\lambda_n C_n=\begin{pmatrix}
0 \\
0\\
\vdots \\
0\\
\end{pmatrix}$$
Ainsi on a  $A\begin{pmatrix}
1 \\
-\lambda_2\\
\vdots \\
-\lambda_n\\
\end{pmatrix}=\begin{pmatrix}
0 \\
0\\
\vdots \\
0\\
\end{pmatrix}$. En multipliant par $A^{-1}$ à gauche, on obtient  $\begin{pmatrix}
1 \\
-\lambda_2\\
\vdots \\
-\lambda_n\\
\end{pmatrix}=\begin{pmatrix}
0 \\
0\\
\vdots \\
0\\
\end{pmatrix}$, d'où la contradiction. 
\item Pour la condition suffisante, voir le chapitre sur les applications linéaires. 
\end{itemize}
\end{Demonstration}

\begin{Proposition}[Caractérisation de l'inversibilité 2]
Soit $A\in\MnK$.\\
La matrice $A$ est inversible si et seulement si pour tout second membre $Y \in \K^n$, le système linéaire $AX =Y$ d'inconnue $X \in \K^n$
possède une unique solution.
\end{Proposition}
\begin{Demonstration}
\begin{itemize}
\item Supposons que $A$ est inversible. $AX =Y $ si et seulement si $A^{-1}(AX)=A^{-1}Y$ si et seulement si $X=A^{-1}Y$. Ainsi l'unique solution du système est $A^{-1}Y.$
\item Supposons que pour tout second membre $Y \in \K^n$, le système linéaire $AX =Y$ d'inconnue $X \in \K^n$. En particulier, pour tout $i\in\Intf{1}{n}$, il existe $C_i$ tel que $A C_i=\begin{pmatrix}0\\\vdots\\\overset{\text{{\tiny \propri{pos i}}}}{1}\\\vdots\\0\end{pmatrix}$. On pose la matrice $B$ des colonnes $C_1,\dots,C_n$. On a :
 $$AB=A\begin{pmatrix}
&   & \\
C_1 & \cdots & C_{n}\\
&  & \\
\end{pmatrix}=\begin{pmatrix}
&   & \\
AC_1 & \cdots & AC_{n}\\
&   & \\
\end{pmatrix}=I_n.$$ Donc $A$ est inversible d'inverse $B$.
\end{itemize}
\end{Demonstration}


\subsection{Calcul de l'inverse}
Une \impo{première méthode} est de supposer l'existence de l'inverse et de déterminer des conditions nécessaires sur les coefficients de la matrice inverse.  
\begin{Exemple}
Démontrons que la matrice $A =\begin{pmatrix}
1& 2\\
0 &3\end{pmatrix}$ est inversible. \\
\textit{Analyse} : Supposons que $A$ est inversible. Donc il existe une matrice $B =\begin{pmatrix}
a& b\\
c &d\end{pmatrix}$ telle que $AB=I_n$, soit :
$$ \begin{pmatrix}
1& 2\\
0 &3\end{pmatrix}\begin{pmatrix}
a& b\\
c &d\end{pmatrix} = \begin{pmatrix}
1& 0\\
0 &1\end{pmatrix}$$
Ainsi,  
$$\begin{pmatrix}
a+2c& b+2d\\
3c &3d\end{pmatrix}= \begin{pmatrix}
1& 0\\
0 &1\end{pmatrix}$$
Par identification, on obtient le système :
$$\begin{cases}
a+2c =1\\
 b+2d=0\\
3c =0\\
3d=1
\end{cases}.$$
Par substitution, on trouve que  $a = 1, b = -\frac 2 3, c = 0, d = \frac 1 3$.\\
\textit{Synthèse} : On vérifie bien que $\begin{pmatrix}
1& 2\\
0 &3\end{pmatrix}\begin{pmatrix}
1& -\frac 2 3\\
0 &\frac 1 3\end{pmatrix}=\begin{pmatrix}
1& 0\\
0 &1\end{pmatrix}$.
\end{Exemple}
\begin{Exemple}
Démontrons que la matrice $A =\begin{pmatrix}
3& 0\\
5 &3\end{pmatrix}$ n'est pas inversible.\\
\textit{Absurde} : Supposons que $A$ est inversible. Donc il existe une matrice $B =\begin{pmatrix}
a& b\\
c &d\end{pmatrix}$ telle que $AB=I_n$, soit :
$$ \begin{pmatrix}
3& 0\\
5 &0\end{pmatrix}\begin{pmatrix}
a& b\\
c &d\end{pmatrix} = \begin{pmatrix}
1& 0\\
0 &1\end{pmatrix}$$
D'où,  
$$\begin{pmatrix}
3a+5b& 0\\
3c+5d &0\end{pmatrix}= \begin{pmatrix}
1& 0\\
0 &1\end{pmatrix}$$
Par identification, on obtient la contradiction $1=0$. Donc $A$ n'est pas inversible.  
\end{Exemple}
Une \impo{seconde méthode} est de résoudre un système linéaire puis  de conclure d'après la seconde caractérisation de l'inversibilité.
\begin{Exemple}
Démontrons que la matrice $A =\begin{pmatrix}
1& 0&1\\
1& 1 &1\\
0& 2 &1\\
\end{pmatrix}$ est inversible. \\
Soit $Y=\begin{pmatrix}
a\\b\\c\\
\end{pmatrix}\in \R^3$.\\
Déterminons l'ensemble des solutions du système $AX=Y$ d'inconnue $X=\begin{pmatrix}
x\\y\\z\\
\end{pmatrix}\in \R^3$. On applique l'algorithme du pivot de Gauss (voir chapitre système linéaire) au système suivant :
$$\begin{aligned}
AX=Y \Leftrightarrow& \left\{\begin{matrix}
x &+&   &+& z&=& a\\
x &+& y &+&z&=&b\\
 && 2y  &+&z&=& c\\
     \end{matrix}\right.\\
     \Leftrightarrow&
     \left\{\begin{matrix}
x &+&   &+& z&=& a&\\
 && y & &  &=&b-a&\quad L_2=L_2-L_1\\
 && 2y  &+&z&=& c&\\
     \end{matrix}
\right.\\
     \Leftrightarrow&
     \left\{\begin{matrix}
x &+&   &+& z&=& a&\\
 && y & &  &=&b-a&\\
 &&  &&z&=& c-2b+2a&\quad L_3=L_3-2L_2\\
     \end{matrix}
\right.\\
     \Leftrightarrow&
     \left\{\begin{matrix}
x &+&   && &=& -a+2b-c&\quad L_1=L_1-2L_3\\
 && y & &  &=&b-a&\\
 &&  &&z&=& c-2b+2a&\\
     \end{matrix}
\right.\\
     \Leftrightarrow&
\begin{pmatrix}
x\\y\\z\\
\end{pmatrix}= \begin{pmatrix}
-1& 2&-1\\
-1& 1 &0\\
2& -2 &1\\
\end{pmatrix}\begin{pmatrix}
a\\
b\\
c\\
\end{pmatrix}
\\
\end{aligned}
$$
La matrice $A$ est donc inversible car le système linéaire $AX=Y$ admet une unique solution et son inverse est :
$$A^{-1}= \begin{pmatrix}
-1& 2&-1\\
-1& 1 &0\\
2& -2 &1\\
\end{pmatrix}.$$
\end{Exemple}


\begin{Proposition}[Matrice de taille 2]
Soit $A=\begin{pmatrix}
a & b\\c& d
\end{pmatrix}$.\\
A est inversible si et seulement si  $ad - bc \neq 0$.\\
Dans ce cas,  on a :
$$ A^{-1}= \frac{1}{ad-bc}\begin{pmatrix}
d & -b\\-c& a
\end{pmatrix}.$$
\end{Proposition}
\begin{Demonstration}
La condition $ad - bc \neq 0$ signifie que les deux colonnes de la matrice ne sont pas colinéaires. D'après la caractérisation de l'inversibilité, la matrice $A$ est inversible si et seulement si  $ad - bc \neq 0$. Si $ad - bc \neq 0$, on a bien : 
$$\begin{pmatrix}
a & b\\c& d
\end{pmatrix}\left(\frac{1}{ad-bc}\begin{pmatrix}
d & -b\\-c& a
\end{pmatrix}\right)=\frac{1}{ad-bc}\begin{pmatrix}
ad-bc & -ab+ab\\cd-dc& -cb+da
\end{pmatrix}=I_2.$$
\end{Demonstration}

\section{Application à l'étude théorique des systèmes linéaires}
Muni de l'objet matrice, nous pouvons démontrer plus facilement les théorèmes du chapitre sur les systèmes linéaires.
\begin{Theoreme}[Structure de l'ensemble des solutions d'un système linéaire]
Soit un système de $n$ équations linéaires à $p$ variables de la forme :
$$\left\{{\begin{matrix}
a_{11}x_{1}&+&a_{12}x_{2}&+&\dots &+&a_{1p}x_{p}&=&b_{1}\\
a_{21}x_{1}&+&a_{22}x_{2}&+&\dots &+&a_{2p}x_{p}&=&b_{2}\\
\vdots&&\vdots&& &&\vdots&=&\vdots \\
a_{i1}x_{1}&+&a_{i2}x_{2}&+&\dots &+&a_{ip}x_{p}&=&b_{i}\\
\vdots&&\vdots&& &&\vdots&=&\vdots \\
a_{n1}x_{1}&+&a_{n2}x_{2}&+&\dots &+&a_{np}x_{n}&=&b_{n}\\
\end{matrix}}\right. $$
Deux cas se présentent :
\begin{itemize}
\item ou bien le système n'admet pas de solution, on dit qu'il est \defi{incompatible}, 
\item ou bien le système admet au moins une solution $(x_1,\dots,x_n)$, appelée \defi{solution particulière}, on dit qu'il est \defi{compatible}. Dans ce cas, les solutions sont de la forme :
$$ S=\{\overbrace{(x_1,\dots ,x_n)}^{\text{Solution particulière}}+ \overbrace{(y_1,\dots,y_p)}^{\text{Solution homogène}}:\quad (y_1,\dots,y_p) \in S_H\}.$$
\end{itemize}
\end{Theoreme}
\begin{Demonstration}
Soit $S$ l'ensemble des solutions du système d'équations.\\ Supposons que les système  possède une solution, noté $X_p$.\\
Soit $A=\begin{pmatrix}
a_{11}&\dots&a_{1p}\\
\vdots &&\vdots \\
a_{n1}&\dots&a_{np}\\
\end{pmatrix}
$ et $Y=\begin{pmatrix}
b_{1}\\
\vdots\\
b_{n}\\
\end{pmatrix}.$\\
On a :
$$
\begin{aligned}
X=\begin{pmatrix}
x_1 \\ \vdots \\ x_d
\end{pmatrix} \in S \Leftrightarrow& \left\{{\begin{matrix}
a_{11}x_{1}&+&a_{12}x_{2}&+&\dots &+&a_{1p}x_{p}&=&b_{1}\\
a_{21}x_{1}&+&a_{22}x_{2}&+&\dots &+&a_{2p}x_{p}&=&b_{2}\\
\vdots&&\vdots&& &&\vdots&=&\vdots \\
a_{i1}x_{1}&+&a_{i2}x_{2}&+&\dots &+&a_{ip}x_{p}&=&b_{i}\\
\vdots&&\vdots&& &&\vdots&=&\vdots \\
a_{n1}x_{1}&+&a_{n2}x_{2}&+&\dots &+&a_{np}x_{n}&=&b_{n}\\
\end{matrix}}\right. \\\Leftrightarrow&  AX=B\overbrace{\Leftrightarrow}^{AX_p=B}  AX=AX_p\Leftrightarrow A(X-Xp)=0\\
\Leftrightarrow& X-Xp\text{ est solution du sytème homogène }\\
\Leftrightarrow& X\text{  est la somme de la solution particulère et d'une solution du système homogène}\\
\end{aligned}
$$
\end{Demonstration}
\begin{Proposition}[Équivalence par opérations élémentaires]
Les opérations élémentaires transforment un système linéaire en un système linéaire équivalent.
\end{Proposition}
\begin{Demonstration}
Soit $Q$ inversible.\\
$X$ est solution d'un système linéaire si et seulement si $AX=Y$ si et seulement si $QAX=QY$.\\
Donc si les matrices  associées aux transformations élémentaires sont inversibles alors  les opérations élémentaires transforment un système linéaire en un système linéaire équivalent. On a :
\begin{itemize}
\item $L_i\leftrightarrow L_j:$ Soit 
$Q_{L_i\leftrightarrow L_j}=
%\begin{pmatrix}
%1 & 0       & \dots  & \dots  &\dots  &\dots&\dots&\dots&\dots  &\dots&0\\
% 0 & \ddots &   &   &  &&&&  &&\vdots\\
%\vdots  &        & 1 &   &  &&&&  &&\vdots\\
%\vdots  &        &   & 0 & \dots &\dots&0&\commentterm{60:0.5cm}{1}{position ij}& &&\vdots\\
%\vdots  &&&\vdots& 1 &        &   & 0  &   &&\vdots\\
%\vdots  &&&\vdots&  & \ddots &   &\vdots   &  &&\vdots\\
%\vdots  &&&0& &        & 1 &   \vdots&   &&\vdots \\
%\vdots  &&&\commentterm{-100:1cm}{1}{position ji} &0 &\dots        & \dots  & 0 &   &&\vdots\\
%\vdots  &&&&  &&&& 1 &        &   \vdots  \\
%\vdots  &&&&  &&&&   & \ddots &    0\\
%0&\dots&\dots&\dots&  \dots&\dots&\dots&\dots&\dots  &\dots        & 1   \\
%\end{pmatrix}\\
\begin{pmatrix}
1 &        &   &   &  &&&&  &&\\
  & \ddots &   &   &  &&&&  &&\\
  &        & 1 &   &  &&&&  &&\\
  &        &   & 0 &  &&&\overset{\text{{\tiny \propri{pos ij}}}}{1}& &&\\
  &&&& 1 &        &   &   &   &&\\
  &&&&  & \ddots &   &   &  &&\\
  &&&& &        & 1 &   &   && \\
  &&&\overset{\text{{\tiny \propri{pos ji}}}}{1} & &       &   &0   &   &&\\
  &&&&  &&&& 1 &        &   \ \\
  &&&&  &&&&   & \ddots &    \\
 &&&&  &&&&  &        & 1   \\
\end{pmatrix}
.$\\
On a $Q_{L_i\leftrightarrow L_j} \begin{pmatrix}
& L_1 & \\
& \vdots & \\
& L_i & \\
& \vdots & \\
& L_j & \\
& \vdots & \\
&  L_n & \\
\end{pmatrix}=\begin{pmatrix}
& L_1 & \\
& \vdots & \\
& L_j & \\
& \vdots & \\
& L_i & \\
& \vdots & \\
&  L_n & \\
\end{pmatrix}.$ Comme $Q_{L_i\leftrightarrow L_j}Q_{L_i\leftrightarrow L_j}=I_n$,$ Q_{L_i\leftrightarrow L_j}$ est inversible et  égale à son inverse.
  
\item $L_i\leftarrow \lambda L_i$ avec $\lambda\neq 0$  : Soit $\lambda\neq 0$ et 
$Q_{L_i\leftarrow \lambda L_i }=
\begin{pmatrix}
1&        &    &&&&\\
 &\ddots  &   &&&&\\
 &        &1  &  &&& \\
 &        &   &\overset{\text{{\tiny \propri{pos ii}}}}{1}&&&\\
 &        &   &      &1 &&\\
 &        &   &      & & \ddots &  \\
 &        &   &      & &  &1    \\
\end{pmatrix} $.\\
On a $Q_{L_i\leftarrow \lambda L_i } \begin{pmatrix}
& L_1 & \\
& \vdots & \\
& L_i & \\
& \vdots & \\
&  L_n & \\
\end{pmatrix}=\begin{pmatrix}
& L_1 & \\
& \vdots & \\
& \lambda L_i & \\
& \vdots & \\
&  L_n & \\
\end{pmatrix}.$ Comme $Q_{L_i\leftarrow \lambda L_i}Q_{L_i\leftarrow \frac 1 \lambda L_i}=I_n$,$ Q_{L_i\leftarrow \lambda L_i}$ est inversible.

\item $L_i\leftarrow  L_i- \lambda L_j $ avec $i\neq j$  : Soit $i\neq j$ et 
$Q_{L_i\leftarrow  L_i- \lambda L_j }=
\begin{pmatrix}
1&        &    &&&&\\
 &\ddots  &   &&&&\\
 &        &1  &  &&& \\
 &        &   &\ddots&&&\\
 &        &\overset{\text{{\tiny \propri{pos ii}}}}{1}   &      &1 &&\\
 &        &   &      & & \ddots &  \\
 &        &   &      & &  &1    \\
\end{pmatrix} $.\\
On a $Q_{L_i\leftarrow \lambda L_i } \begin{pmatrix}
& L_1 & \\
& \vdots & \\
& L_i & \\
& \vdots & \\
&  L_n & \\
\end{pmatrix}=\begin{pmatrix}
& L_1 & \\
& \vdots & \\
& \lambda L_i & \\
& \vdots & \\
&  L_n & \\
\end{pmatrix}.$ Comme $Q_{L_i\leftarrow \lambda L_i}Q_{L_i\leftarrow \frac 1 \lambda L_i}=I_n$,$ Q_{L_i\leftarrow \lambda L_i}$ est inversible.
\end{itemize} 
\end{Demonstration}
\begin{Remarque}
La preuve de cette proposition nous donne une méthode pour déterminer si une matrice est inversible ou non et calculer son inverse le cas échéant. En effet, soit $A\in\MnK$. D'un point de vue matricielle l'algorithme du pivot de Gauss permet de transformer $A$ par des opérations élémentaires $Q_1,\dots,Q_r$, c'est à dire :
 $(Q_1\dots Q_r)A$. Deux cas de figures se présentent à la fin de l'algorithme, soit on obtient la matrice identité, soit on obtient 
\begin{itemize}
\item  Comme les opérations élémentaires sont des matrices inversibles, $(Q_1\dots Q_r)$ est inversible. Si 
\end{itemize}
%D'un point de vue matricielle l'algorithme du pivot de Gauss permet de transformer $A$ en $I$ par des opérations élémentaires $Q_1,\dots,Q_r$, c'est à dire :
% $(Q_1\dots,Q_r)A=I_n$. Ainsi la matrice $A$ est inversible d . 
%À présent, faisons l?hypothèse que nous avons réussi à transformer A en In par des opérations élémentaires P1, . . . , Pr
%sur les LIGNES, dans cet ordre. Matriciellement, cela revient à dire que : Pr . . . P1A = In ? multiplications à
%GAUCHE. Or les matrices P1, . . . , Pr sont inversibles, donc : A= P?1
%1 . . . P?1
%r , et donc A est inversible. Que vaut son
%inverse ? Tout simplement : A?1 = Pr . . . P1 = Pr . . . P1 In. Conclusion inattendue : les mêmes opérations qui ont
%transformé A en In permettent de transformer In en A?1.
\end{Remarque}



\section{Annexe}
\subsection{Matrice par blocs}
\begin{Definition}[Décomposition par blocs]
Soit $A\in  \MnpK $ une matrice.\\
Sa \defi{décomposition par blocs} suivant le découpage $(n_1,\dots,n_s) $ pour les lignes et $(p_1,\dots,p_t)$ pour les colonnes est
\begin{center}
  \begin{tikzpicture}[
      dots/.style={
        line width=1pt,
        line cap=round,
        dash pattern=on 0pt off 5pt,
        shorten >=.1cm,
    shorten <=.1cm}]
    \matrix[
      matrix of nodes,
      left delimiter=(,
      right delimiter=),
    ]{
      \node (A) {$A_{1,1}$}; &[1.1cm] \node (B) {$A_{1,t}$}; \\[1.1cm]
      \node (C) {$A_{s,1}$}; &        \node (D) {$A_{s,t}$}; \\
    };

    \draw [dots] (A.east)  -- (B.west);
    \draw [dots] (C.east)  -- (D.west);
    \draw [dots] (A.south) -- (C.north);
    \draw [dots] (B.south) -- (D.north);

    \draw [<->]  ([xshift=1cm]B.north east) -- node [right] {$n_1$} ([xshift=1cm]B.south east);
    \draw [<->]  ([xshift=1cm]D.north east) -- node [right] {$n_s$} ([xshift=1cm]D.south east);
    \draw [dots] ([xshift=1cm]B.south east) -- ([xshift=1cm]D.north east);
    \draw [<->]  ([xshift=1.75cm]B.north east) -- node [right] {$n$} ([xshift=1.75cm]D.south east);

    \draw [<->]  ([yshift=-0.5cm]C.south west) -- node [below] {$p_1$}([yshift=-0.5cm]C.south east);
    \draw [<->]  ([yshift=-0.5cm]D.south west) -- node [below] {$p_t$}([yshift=-0.5cm]D.south east);
    \draw [dots] ([yshift=-0.5cm]C.south east) -- ([yshift=-0.5cm]D.south west);
    \draw [<->]  ([yshift=-1.1cm]C.south west) -- node [below] {$p$} ([yshift=-1.1cm]D.south east);

    \node[xshift=-0.9cm] at ($(A.west)!0.5!(C.west)$) { $A =$ };
  \end{tikzpicture}
\end{center}
\end{Definition}
\begin{Exemple}
La matrice ${P} ={\begin{pmatrix}1&1&2&2\\1&1&2&2\\3&3&4&4\\3&3&4&4\end{pmatrix}}$
peut être partitionnée en quatre blocs $2\times 2$ avec $$P_{11}={\begin{pmatrix}1&1\\1&1\end{pmatrix}},P_{12}={\begin{pmatrix}2&2\\2&2\end{pmatrix}},P_{21}={\begin{pmatrix}3&3\\3&3\end{pmatrix}},{P}_{22}={\begin{pmatrix}4&4\\4&4\end{pmatrix}}.$$
On peut alors écrire la matrice par bloc comme :
$$P={\begin{pmatrix} {P} _{11}& {P} _{12}\\ {P} _{21}& {P} _{22}\end{pmatrix}}.$$
\end{Exemple}
\begin{Proposition}[Addition par blocs]
Soit $A$ et $B$ deux matrices de $\MnpK$ exprimées par blocs selon les mêmes découpages. Soit $\lambda ,\mu  \in  \K $.\\
Alors la matrice $\lambda A + \mu  B$ s'exprime par blocs selon les mêmes découpages que $A$ et $B$, et les blocs s'obtiennent en combinant les blocs situés aux mêmes places.
\end{Proposition}
\begin{Proposition}[Produit par blocs]
Soit $A\in  \MnpK$ et $B\in  \M{p}{q}{\K}$.\\
On suppose que $A$ a une décomposition par blocs suivant le découpage $(n_1,\dots,n_s)$ pour les lignes et $(p_1,\dots,n_t)$ pour les colonnes.\\
On suppose que $B$ a une décomposition par blocs suivant le découpage $(p_1,\dots,n_t)$ pour les lignes et $(q_1,\dots,q_u)$ pour les colonnes.\\
Alors le produit $C = AB\in  \mathrm{M}_{n,q}(\K )$ admet une décomposition par blocs suivant le découpage $(n_1,\dots,n_s)$ pour les lignes et $(q_1,\dots,q_u)$ pour les colonnes
$$ C = \begin{pmatrix} C_{1,1} &  \dots &  C_{1,u}  \\  \vdots &   &  \vdots  \\  C_{s,1} &  \dots &  C_{s,u} \end{pmatrix} $$
où pour tous $i\in  \Intf{1}{s}$ et $j\in  \Intf{1}{u}$,
$$ C_{i,j} = \sum  _{k=1}^t A_{i,k} B_{k,j} $$
Cette formule est identique au produit matricielle en considérant les blocs comme des scalaires.
\end{Proposition}
\begin{Remarque}
Le cas qui nous intéressera le plus fréquemment est celui des matrices carrées ayant le même découpage pour les lignes et pour les colonnes. Dans ce cas, les blocs diagonaux sont également des matrices carrées.
\end{Remarque}

\end{document}
