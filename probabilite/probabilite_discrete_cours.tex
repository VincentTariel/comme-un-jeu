\documentclass[a4paper]{book}
\usepackage{t1enc}
\usepackage[latin1]{inputenc}
\usepackage[french]{minitoc}
 \usepackage{amsmath}
\usepackage{fancyhdr,amsmath,amsthm,amssymb,fancybox}
\usepackage[francais]{babel}
\usepackage{amsmath}
\usepackage{TikZ}
\usetikzlibrary{shapes,backgrounds}
\usepackage{tkz-fct}   
\usepackage{a4wide,jlq2eams} 
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{slashbox}
\usepackage{thmbox}
\usepackage{xcolor}
\usepackage{sectsty}
\usepackage{longtable} 
\definecolor{amaranth}{rgb}{0.9, 0.17, 0.31}
\sectionfont{\color{magenta}}
\subsectionfont{\color{red}}
\subsubsectionfont{\color{red}}
\newcommand{\defi}[1]{\textbf{\textcolor{orange}{#1}}}

\setlength{\shadowsize}{1.5pt}
 
\pagestyle{fancy}
\addtolength{\headwidth}{\marginparsep}
\addtolength{\headwidth}{\marginparwidth} 
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\fancyhf{}
\fancyhead[LE,RO]{\bfseries\thepage}
\fancyhead[LO]{\bfseries\rightmark}
\fancyhead[RE]{\bfseries\leftmark}
\fancypagestyle{plain}{%
   \fancyhead{} % get rid of headers
   \renewcommand{\headrulewidth}{0pt} % and the line
}

\setcounter{minitocdepth}{3}



\thmboxoptions{S,bodystyle=\itshape\noindent}
\newtheorem[L]{Lem}{Lemme}[section]
\newtheorem[L]{Th}[Lem]{Théorème}
\newtheorem[L]{Cor}[Lem]{Corollaire}
\newtheorem[L]{Prop}[Lem]{Proposition}

\newtheorem[S,bodystyle=\upshape\noindent]{Df}{Définition}
\newtheorem[S,bodystyle=\upshape\noindent]{Ex}{Exemple}
\newtheorem[S,bodystyle=\upshape\noindent]{NB}{Remarque}
\newtheorem[S,bodystyle=\upshape\noindent]{intr}{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\E}{(-4,-1) rectangle (4,4);\node[above right] at (-4,-1) {$\Omega$};}
\newcommand{\A}{(0,0) ++(135:2) circle (2);\node at (-1.4,-0.75) {$A$};}
\newcommand{\B}{(0,0) ++(45:2) circle (2);\node at (1.4,-0.75) {$B$};}
\newcommand{\AuB}{(0,0) arc(-135:135:2) arc(45:315:2);\node at (0,1.5) {$A\cup B$};}
\newcommand{\AnB}{(0,0) arc (-45:45:2) arc (135:225:2);\node at (0,1.5) {$A\cap B$};}



\begin{document}
\chapter{Probabilité discrète}
Lors du cours sur les probabilité finies, nous avons posé le cadre probabiliste afin de modéliser une expérience aléatoire dans les issues sont finies. Ce cours étend le cadre pour aussi modéliser une expérience aléatoire dont les issues sont \textbf{dénombrables finies ou infinies}, c'est à dire que l'on peut effectuer une énumération finie ou infinie des issues de l'expérience aléatoire.  Nous énoncerons uniquement les extensions et les nouvelles notions.   
 

\section{Extension  de la modélisation probabilistes pour un univers dénombrable}
\subsection{Axiomes des probabilité. Premières propriétés}
\begin{Ex}[Première fois pile]
Pour le lancer d'une pièce jusqu'à la première obtention de pile. L'univers est dénombrable $\N$ où une issue $\omega\in \N$ représente le temps du premiers succès.
\end{Ex}
\begin{Df}[Réunion et intersection dénombrables]
Soit $(A_n )_{n\in \N}$ une suite d'événements de $\Omega$. On définit les ensembles $\bigcup\limits_{n\in\N}A_n$ et $\bigcap\limits_{n\in\N}A_n$ par :
$$ \omega  \in \bigcup\limits_{n\in\N}A_n \Leftrightarrow \exists n\in\N:\quad \omega \in A_n$$ 
et 
$$ \omega  \in \bigcap\limits_{n\in\N}A_n \Leftrightarrow \forall n\in\N:\quad \omega \in A_n$$ 
\end{Df}
\begin{Ex}[Première fois pile]
L'événement $A_p$="l'expérience se termine après le p-ième lancer" est le singleton $\{p\}$.\\
L'évènement "l'expérience se termine après le p-ième lancer" s'écrit $\bigcup\limits_{n=p}^{+\infty}A_p$.\\
L'évènement "l'expérience se termine après un temps infini" s'écrit $\bigcap\limits_{n=1}^{+\infty} \bar A_p$.
\end{Ex}
\begin{Df}[Axiomes des tribus]
La probabilité $P$  est une fonction qui à un événement associe
un nombre compris entre 0 et 1 quantifiant la chance de réalisation
de cet événement. Pour des raisons sortant du cadre de ce cours, il n'est pas
toujours possible d'attribuer ainsi de manière cohérente une probabilité à
chaque partie de $\Omega$.\\
En d'autres termes, $P$ ne peut pas être considérée comme une application de $\mathcal{P}(\Omega)$
 dans $[0, 1$] mais comme une fonction ayant un domaine de définition $\mathcal{A}$ généralement plus
petit que , noté $\mathcal{P}(\Omega)$.\\
Voici les propriétés qu'il est raisonnable d'exiger de $\mathcal{A}$ :
\begin{itemize}
\item  $\mathcal{A}$ contient $\emptyset$ (et tous les singletons $\{\omega\}$),
\item $\mathcal{A}$ est stable par passage au complémentaire : si $A$ est un événement
de $\mathcal{A}$,  $\mathcal{A}^c$ l'est aussi,
\item $\mathcal{A}$ est stable par les opérations de réunion et d'intersection sur les suites
d'événements 
\item Si $A_1,A_2,\ldots$ est une suite finie ou infinie d'événements
de $\mathcal{A}$, sa réunion et son intersection sont encore des événements de $\mathcal{A}$.
\end{itemize}
Nous appellerons \defi{tribu} toute famille  $\mathcal{A}$ de parties de  vérifiant les conditions ci-dessus.\\
$(\Omega,\mathcal{A})$ est appelé \defi{espace probabilisable}. L'objectif de ce cours n'est pas de savoir construire mathématiquement la tribu  $\mathcal{A}$. On supposera que l'on sait construire à partir de l'univers la tribu ayant les "bonnes propriétés". 
\end{Df}
\begin{Df}[Axiomes des probabilités]
Soit $(\Omega,\mathcal{A})$ un espace probabilisable.
Une \defi{probabilité} sur $(\Omega,\mathcal{A})$ est une application
de $\mathcal{A}$ dans $\R^+$ vérifiant :
\begin{itemize}
\item
  $P(\Omega)=1$.
\item
  Pour toute suite $(A_n)_{n\in\N^*}$ d'événements de $\mathcal{A}$ deux à deux disjoints (incompatibles) :
$$P(\bigcup_{n\in\N^*}A_n)=\sum_{n=1}^{+\infty}P(A_n)$$  
\end{itemize}
Le triplet  $(\Omega,\mathcal{A},P)$ s'appelle un \defi{espace probabilisé}.
\end{Df}
\begin{Prop}
Une probabilité $P$ sur un univers dénombrable est complètement déterminée par les $P(\{\omega\})$ pour tout $\omega \in \Omega$. $P(\{\omega\})$, appelé poids de probabilité.\\
En effet, pour $A \subset \Omega$, on a :
\begin{align*}
P(A)&=P(\bigcup\limits_{\omega\in A}\{w\} ),\\
 &=\sum_{\omega\in A}P(\{w\}).
\end{align*}
\end{Prop}
\begin{Ex}[Première fois pile]
Soit $p$ la probabilité d'obtenir pile. On pose $p_k=P(\{k\})=\overbrace{(1-p)^{k-1}}^{\text{proba d'obtenir d'abord k-1 piles}}p. $. On doit vérifier que $\sum_{k=1}^{+\infty}p_k=1$. 
On a $$\sum_{k=1}^{+\infty}p_k=\sum_{k=1}^{+\infty} (1-p)^{k-1} p = p \sum_{k=1}^{+\infty} (1-p)^{k-1} \overbrace{=}^{\text{somme d'une série géométrique}}=p.\frac{1}{1-(1-p)}=1.$$
\end{Ex}
\begin{Prop}
\begin{enumerate}
\item $ P(\emptyset)=0.$
\item Pour toute suite finie $(A_i)_{1\leq i \leq n}$ d'événements de $\mathcal{A}$ deux à deux disjoints (incompatibles) :
$$P(\bigcup_{1\leq i \leq n}A_i)=\sum_{i=1}^{n}P(A_i).$$
\item Pour tout $A,B\in \mathcal{A}$ tel que  $A\subset B$, on a :
$$P(A)\leq P(B)$$
$$P(B\setminus A )=P(B)-P(A)$$
\item Pour tout $A\in \mathcal{A}$, on a $P(A)\in[0,1]$.
\item Pour tout $A,B\in \mathcal{A}$, on a $$P(A\cup B)+P(A\cap B)=P(A)+P(B).$$
\item Pour tout $A,B\in \mathcal{A}$, on a $$P(A\cap B)\leq P(A)+P(B).$$
\item Pour tout $A\in \mathcal{A}$, on a 
$$P(A)=1-P(A^c).$$
\item \defi{Continuité monotone séquentielle :}
\begin{enumerate}
\item  Soit $(A_n)_{n\in\N^*}$ est une suite croissante d'événements de $\mathcal{A}$, c'est à dire $A_n\subset A_{n+1}$, on a
$$P(\bigcup_{n\in\N^*}A_n)=\lim\limits_{n \to +\infty} P(A_n)$$ 
\item Soit $(A_n)_{n\in\N^*}$ est une suite décroissante d'événements de $\mathcal{A}$, c'est à dire $A_{n+1}\subset A_{n}$, on a
$$P(\bigcap_{n\in\N^*}A_n)=\lim\limits_{n \to +\infty} P(A_n)$$ 
\end{enumerate}
\item \defi{Sous additivité} : Pour toute suite finie $(A_i)_{1\leq i \leq n}$ d'événements de $\mathcal{A}$, on a  :
$$P(\bigcup_{n\in\N^*}A_n)=\sum_{n=1}^{+\infty} P(A_n).$$
\end{enumerate}
\end{Prop}
\begin{proof}
\begin{enumerate}
\item Soit la suite $(A_n)_{n\in\N^*}$ définit par $A_1=\Omega$ et $A_n=\emptyset$ si $n>1$. Les événements de cette suite sont deux à deux disjoints et $\bigcup_{n\in\N^*}A_n=\Omega$.
\begin{align*}
P(\Omega)=&P(\bigcup_{n\in\N^*}A_n)\\
P(\Omega)=&P(\Omega)+P(\emptyset)+P(\emptyset)+\cdots \text{ car deux à deux disjoints}\\
0=&P(\emptyset)+P(\emptyset)+\cdots\\
P(\emptyset)=&0.
\end{align*}
\item Soit la suite $(B_i)_{i\in\N^*}$ définit par $B_i=A_i$ si $i\leq n$ et $B_i=\emptyset$ si $i>n$. On a :
\begin{align*}
P(\bigcup_{1\leq i \leq n}A_i)=&P(\bigcup_{i\in\N^*}B_i)\\
P(\bigcup_{1\leq i \leq n}A_i)=&\sum_{n=1}^{+\infty}P(B_n)\\
P(\bigcup_{1\leq i \leq n}A_i)=&P(A_1)+\dots+P(A_n)+P(\emptyset)+\dots \\
P(\bigcup_{1\leq i \leq n}A_i)=&\sum_{i=1}^{n}P(A_i)
\end{align*}
\item Comme  $A \bigcup\limits_{\text{disjoints}}(B\setminus A)=B$, on a :
$$ P(A)+P(B\setminus A)=P(B).$$
\item Comme $A\subset \Omega$, on a $P(A)\leq P(\Omega)=1$.
\item Comme $A\cup B=(A\setminus (A\cap B))\bigcup\limits_{\text{disjoints}}B$, on a $P(A\cup B)=P(A\setminus (A\cap B))+P(B).$ Comme $(A\cap B)\subset A$, on a bien : 
 $P(A\cup B)=P(A)- P(A\cap B)+P(B).$
\item Comme $P(A\cup B)+ P(A\cap B)=P(A)+P(B)$, on a $P(A\cup B)\leq P(A)+P(B)$.
\item Comme $A \bigcup\limits_{\text{disjoints}}A^c=\Omega$, on a 
$$P(A)+P(A^c)=1.$$
\item \defi{Continuité monotone séquentielle :}
\begin{enumerate}
\item  Soit $(A_n)_{n\in\N^*}$ est une suite croissante d'événements de $\mathcal{A}$. On pose la suite $(B_n)$ défini $B_0=A_0$ et $B_n =A_{n}\setminus A_{n-1}$ pour tout $n\geq 1$. Les éléments de cette suite sont deux à deux disjoints et $\cup_{k=0}^n A_k = \cup_{k=0}^n B_k$. On a :
\begin{align*}
P(\cup_{k=0}^n A_k)=&P(\cup_{k=0}^n B_k)\\
=& \sum_{k=0}^n P(B_k)
\end{align*}
La série numérique $\sum  P(B_n)$ est à termes positifs et majoré par 1 $\left(\sum_{k=0}^n P(B_k)=P(\cup_{k=0}^n B_k)\leq P(\Omega)=1\right)$ donc convergente. Par passage à la limite on a 
\begin{align*}
\lim\limits_{n\to \infty}P(A_n) &=\lim\limits_{n\to \infty} P(\cup_{k=0}^n B_k)\\
&=\sum_{k=0}^{+\infty} P(B_k)\\
&=P(\cup_{k=0}^{+\infty} B_k)\\
&=P(\cup_{k=0}^{+\infty} A_k)
\end{align*}
\item On a :
\begin{align*}
P(\bigcap_{n\in\N^*}A_n)&=1-P((\bigcap_{n\in\N^*}A_n)^c)\\
&=1-P(\bigcup_{n\in\N^*}A_n^c)\\
&=1-\lim\limits_{n\to \infty}P(A_n^c)\\
&=\lim\limits_{n\to \infty}(1-P(A_n^c))\\
&=\lim\limits_{n\to \infty}P(A_n)
\end{align*}
\end{enumerate}
\item \defi{Sous additivité} : Pour toute suite finie $(A_i)_{1\leq i \leq n}$ d'événements de $\mathcal{A}$, on a  :
$$\forall n \in \N :\quad P(\bigcup_{i=1}^n A_n)\leq \sum_{i=1}^{n} P(A_i)\underbrace{\leq}_{\text{termes positifs} } \sum_{i=1}^{+\infty} P(A_i).$$
De plus $\bigcup_{n=1}^{+\infty} A_n= \bigcup_{n=1}^{+\infty} (\cup_{i=1}^n A_i)$ et la suite $(\cup_{i=1}^n A_i)$ est croissante d'où :
$$P(\bigcup_{n=1}^{+\infty} A_n) \underbrace{=}_{\text{Continuité monotome}}\lim\limits_{n\to \infty}P(\bigcup_{i=1}^n A_n)\leq  \lim\limits_{n\to \infty} \sum_{i=1}^{n} P(A_i).$$ 
\end{enumerate}
\end{proof}


\begin{Prop}[Formule des probabilités totales]
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.\\
Soit $ (A_n)_{n\in \N*}$ un système complet d'événements de probabilités non nulles.\\
Pour tout événement $B$, on a
\[ P(B) = \sum_{i=1}^{+\infty} P_{A_i}(B)P(A_i). \]
\end{Prop}
\begin{Prop}[formule de Bayes usuelle]
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.
Soit $(A_n)_{n\in \N*}$ un système complet d'événements de probabilités non nulles.
Pour tout événement $B$ de probabilité non nulle,
et pour tout $k\in\{1,n\}$, on a
\[ P_B(A_k) = \frac{ P_{A_k}(B)P(A_k) }{ \sum\limits_{i=1}^{+\infty} P_{A_i}(B)P(A_i) }. \]
\end{Prop}



\section{Variables aléatoire discrètes}

\subsection{Définition}
\begin{Ex}[Martingale Roulette]La Roulette, inventée par Blaise Pascal, est un des jeux les plus populaires des casinos. Le pari simple est de parier la couleur de la case sur laquelle la bille lancée dans le cylindre va s'arrêter parmi 37 cases (18 rouges, 18 noirs et un vert).\\
La martingale est : le joueur doit, suite à chaque perte, tripler sa mise. S'il perd à nouveau, il mise la somme initiale triplée et ainsi de suite, jusqu'à ce qu'il gagne. Par exemple, le joueur joue 1 euro à la première partie et perd. Puis il joue donc 3 euros et  perd encore. Après, il joue donc 9 et gagne. Finalement, son gain est $9-3-1=5$ euros.\\  
A votre avis, ce jeu est-il favorable au joueur ou au casino ? A chaque partie quelle est le gain moyen du joueur ?\\
\'Etape 1 : comme pour l'expérience aléatoire du lancer d'une pièce jusqu'à la première obtention de pile, on pose $X$ la variable aléatoire représentant le nombre de partir joué jusqu'à gagner. Par exemple, $X=3$ signifie que l'on a perdu les deux premières parties et gagné la troisième. La loi de la variable aléatoire est $X$ est :
$$P(X=n)=(1-p)^{n-1} p\text{ avec } p=\frac{18}{37}.$$
\'Etape 2 :  on calcule nos gains en fonction de $x$ .
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|c|}
\hline
$x$ & 1 & 2  & 3&       4        & \dots & $n$& \dots  \\\hline
$g$ & 1 & 3-1& 9-(1+3)&27-(1-3-9)&\dots  &$3^{n-1}-\sum_{k=0}^{n-2}3^k=\frac{3^{n-1}+1}{2}$& \\\hline
\end{tabular}
 \end{center}
Le gain moyen est donc :
$$E(G(X))=\sum_{n=1}^{+\infty} \frac{3^{n-1}+1}{2}  (1-p)^{n-1} p=\frac p 2 \sum_{n=1}^{+\infty} (3^{n-1}+1)  (1-p)^{n-1}$$
La série géométrique $\sum (1-p)^{n-1}$ converge car $|1-p|<1$. Par contre, la série numérique  $\sum (3(1-p))^{n-1}$ diverge vers l'infini car $|3(1-p)|>1$. En conclusion, $E(G(X))=+\infty$ et donc ce jeu est "très" favorable au joueur. L'unique condition pour jouer à ce jeu est de n'avoir pas de limite sur son compte en banque$\dots$.  
\end{Ex}
\begin{Df}[Variable aléatoire réelle discrète]
Soit $(\Omega,\mathcal{A}(\Omega))$ un espace probabilisable.\\
Une \defi{variable aléatoire réelle discrète} est une application : $$\Fonction{X}{\Omega}{\R}{\omega}{X(\omega)}.$$ telle que
\begin{enumerate}
\item $X (\Omega)$ est un ensemble dénombrable;
\item Si $x \in X (\Omega)$ alors $X^{-1}(\{x \}) = (X = x )$ est un événement, c'est-à-dire :$X^{-1}(\{x \})\in \mathcal{A}$.
\end{enumerate}
\end{Df}
\begin{NB}
Lors de la définition de l'axiome des tribus, l'ensemble des événements ne devait ne pas être "trop grand" afin de donner du sens à l'application probabilité. La définition de la variable aléatoire impose que l'ensemble des événements ne soit pas être "trop petit".
\end{NB}

\begin{Prop}
Soit $(\Omega,\mathcal{A})$ un espace probabilisable et $X$ une variable aléatoire discrète sur cet espace.\\
Soit $A$ une partie de $\R$. L'ensemble $X^{-1}(A)=\{X \in A\}$, est un événement.
\end{Prop}
Comme toutes les variables aléatoires sont supposées discrètes, on s'autorise désormais à écrire l'ensemble des images sous la forme $X (\Omega) = \{x_n:n\in \N\}$.
\begin{Prop}
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé et $X$ une variable aléatoire discrète sur cette espace.\\
L'ensemble $(X=x_i)_{n\in\N}$ forme un système complet d'événements de $\Omega$.
\end{Prop}
\subsection{Loi d'une variable aléatoire}
\begin{Df}[Loi d'une variable aléatoire]
Soit $X$ une variable aléatoire discrète sur un espace probabilisé $(\Omega,\mathcal{P}(\Omega),P)$.\\ 
L'application
\[ \Fonction{P_X}{X(\Omega)}{[0,1]}{x}{P(X=x)} \]
est une probabilité sur $\R$, appelée \defi{loi de la variable $X$}.
\end{Df}



\begin{Th}
Soit $\{x_n:n\in\N\}$ une famille dénombrable de réels deux à deux distincts et $\{p_nn\in\N:\}$ une famille de réels positifs telle que $\sum_{n=1}^{\infty} p_n = 1$. Alors il existe 
une variable aléatoire discrète $X$ sur un
espace probabilisé $(\Omega ,\mathcal{A} ,P)$ à valeurs dans  $\{x_n:n\in\N\}$ telle que : 
$$ \forall n \in \N:\quad  P( X = x_n ) = p_n.$$
\end{Th}
\subsection{Fonction de répartition}
\begin{Df}[Fonction de répartition]
Soit $X$ une variable aléatoire discrète sur un espace probabilisé $(\Omega,\mathcal{P}(\Omega),P)$.\\ 
On appelle fonction de
répartition de $X$ et on note $F_X$ l'application :
\[ \Fonction{F_X}{\R}{\R}{x}{P(X\leq x)}.\]
\end{Df}


\begin{Df}[Propriétés de la fonction de répartition]
Soit $X$ une variable aléatoire discrète sur un espace probabilisé $(\Omega,\mathcal{P}(\Omega),P)$.\\ 
\begin{enumerate}
\item L'application $F_X$ est croissante.
\item $\lim\limits_{x\to-\infty}F_X(x)=0$.
\item $\lim\limits_{x\to+\infty}F_X(x)=1$.

\end{enumerate}
\end{Df}

\subsection{Espérance}
Pour donner du sens à l'espérance $\sum_{x\in X(\Omega)}x P(X = x )$ dans le cas d'une variable discrète, nous sommes confrontés à
deux difficultés :
\begin{enumerate}
\item  $X(\Omega)$ étant au plus dénombrable, on peut l'écrire sous la forme $\{x_n | n \in \N\}$. La série numérique $\sum x_n P(X = x_n )$ converge-t-elle ?
\item La somme ne dépend-elle pas du choix de la bijection entre $X(\Omega)$ et $\N$, autrement dit de l'ordre de
sommation ?
\end{enumerate}
La réponse aux deux questions est oui, du moment que la série est supposée absolument convergente.




\begin{Ex}[Bayes]
Un magasin possède $n$ caisses. Les clients se répartissent de façon indépendante et équiprobable entre
les différentes caisses. On suppose que la probabilité qu'il y ait $k$ clients dans le magasin est $p_k = e^{-\lambda}\frac{\lambda^k}{k!}$.\\
La caisse numéro 1 a vu passer $m$ clients un jour donné.\\
Quelle est la probabilité qu'il y ait eu dans le magasin $n m$ clients ?

\end{Ex}




\subsection{Convergence et approximations}

\begin{Prop}[Approximation d'une loi binomiale par une loi de Poisson]
Soit $(X_n)_{n\in\N}$ une suite de variables aléatoires tel que $X_n\hookrightarrow \mathcal{B}(n,p_n)$ et $\lim\limits_{n\to\infty}np_n=\lambda$.\\
Alors on a :
$$\forall k \in \N:\quad  \lim\limits_{n\to\infty} P(X_n = k) = e^{-\lambda}\frac{\lambda^k}{k!}.$$
\end{Prop}
\begin{proof}
Soit $k\in \N$. On a :
\begin{align*}
P(X_n = k) &= \begin{pmatrix}n \\ k \end{pmatrix} (p_n)^k (1-p_n)^{n-k}\\
&= \frac{1}{k!} n.(n-1).\dots (n-k+1)(p_n)^k (1-p_n)^{n-k} \\
&= \frac{1}{k!} 1.(1-1/n).\dots (1-(k+1)/n)(p_n/n)^k (1-p_n)^{n-k} \\
\end{align*}
D'une part  $\overbrace{1.(1-1/n).\dots (1-(k+1)/n)}^{k \text{ facteurs fixes}}\tend[n\to+\infty]1$, d'autre part $(p_n/n)^k\tend[n\to+\infty] \lambda^k$ et enfin $(1-p_n)^{n-k}=e^{(n-k)\ln(1-p_n)}\tend[n\to+\infty] e^{-\lambda}$ car $(n-k)\ln(1-p_n)\equivalent[n\to+\infty] -np_n$. Finalement
$$\lim\limits_{n\to\infty} P(X_n = k) = e^{-\lambda}\frac{\lambda^k}{k!}.$$
\end{proof}
%TODO DEMO

%TODO loi faible des grands nombres
% Rajouter 
En pratique, si $X$ est une variable aléatoire suivant la loi binomiale $\mathcal{B}(n,p)$ avec $n\geq 30$, $p\leq 0,10$ et $np\geq 15$, on peut approximer la loi de $X$ par la loi de Poisson de paramètre $np$.\\
Ce théorème  justifie le fait que la loi de Poisson est utilisée comme modèle de certaines expériences aléatoires (nombre de clients entrant dans un magasin, nombre de coquilles dans une page de journal,...).

\begin{Prop}[Inégalité de Markov]
Soit $X$ une variable aléatoire réelle positive.\\
Pour tout $a > 0$, on a :
\[ P(X\geq a) \leq \frac{E(X)}{a}.\]
\end{Prop}
\begin{Prop}[Inégalité de Bienaymé-Tchebychev]
Soit $X$ une variable aléatoire réelle.
Pour tout $a>0$, on a
\[ E( |X-E(X)| \geq a ) \leq \frac{V(X)}{a^2} \]
\end{Prop}
%TODO FAIRE LES DEMOS
Cette inégalité présente un intérêt théorique en majorant la
probabilité qu'une variable aléatoire s'écarte de sa moyenne. Nous allons l'utiliser pour prouver la loi faible des grands
nombres.




Lors d'un lancer d'une pièce de monnaie équilibrée, les deux côtés « pile » et « face » apparaissent de façon équiprobable pour des raisons de symétrie : on ne s'attend pas plus à l'un ou à l'autre côté. Cette mesure de l'attente s'appuie souvent sur une considération statistique : on observe que la fréquence des occurrences de chaque côté se rapproche de 1/2. 
\begin{center}
\includegraphics[width=8cm]{frequence_lancer.png}
\end{center}
Le théorème de la loi des grands nombres permet de justifier ce résultat en interprétant la probabilité comme une fréquence de réalisation.\\
La modélisation de cette expérience aléatoire est :
\begin{enumerate}
\item  \textbf{n\up{ième} lancer} :  pour tout $n\in\N^*$,  la variable aléatoire $X_n$, représente le résultat du n\up{ième} lancer. Elle  suit une loi Bernouilli de paramètre $p$ (l'issue 0 représente le pile et l'issue 1 le face). Elles sont supposées indépendantes.  On a $E(X_n) = (1 - p).0 + p.1  = p.$ 
\item \textbf{fréquence} : pour tout $n\in\N^*$,  la variable aléatoire $S_n$, représente la moyenne des résultats obtenus au cours des n premiers lancers, soit :
$$ S_n =\frac{X_1+X_2+\dots+X_n}{n}$$ 
\end{enumerate}
L'issue, $\omega$, correspondant à une succession de faces existe, $X_n(\omega)=1$ pour tout $n\in \N^*$. Donc $1$ est une valeur possible de la variable aléatoire $S_n$. Cependant, le théorème de la loi faible des grands nombres prouve que la probabilité que $S_n$ s'écarte de l'espérance $E(X_n)$ tend vers 0 quand $n$ tend vers l'infini. 
\begin{Th}[Loi faible des grands nombres]
Soit $(X_n)_{n\in\N^*}$ une suite de variables aléatoires discrètes définies sur un espace probabilisé $(\Omega,\mathcal{A} ,P)$. On
suppose toutes les variables indépendantes et de même loi, admettant une espérance $m$ et un écart
type $\sigma$. Posons $S_n =\frac{X_1+X_2+\dots+X_n}{n}.$\\
Alors 
$$ \forall \epsilon>0: \quad P\left(\left| S_n - m \right|\geq \epsilon \right )\tend[n\to+\infty]0.$$
\end{Th}
\begin{proof}
D'après l'inégalité de Bienaymé-Tchebychev et par indépendance des variables aléatoires,
$$ \forall \epsilon>0: \quad P\left(\left| S_n - m \right|\geq \epsilon\right)\leq \frac{V(S_n)}{\epsilon^2}=\frac{\frac{V(X_1+X_2+\dots+X_n)}{n^2}}{\epsilon^2}=\frac{\sigma^2}{n\epsilon^2} \tend[n\to+\infty]0.$$
\end{proof}
\begin{NB}
Le TP Python \url{https://github.com/VincentTariel/cours/blob/master/probabilite/simulation_variable_aleatoire_avtivite_python.pdf} permet de vous familiariser avec ce théorème sur des applications.
\end{NB}

\end{document}
