\section{Modélisation probabilistes pour un univers fini}
\subsection{Axiomes des probabilité. Premières propriétés}
\begin{Df}[Univers]
Un \defi{univers}, noté $\Omega$, est l'ensemble de toutes les \defi{issues} (résultats) qui peuvent être obtenues au cours d'une expérience aléatoire. Une issue est noté $\omega$. Dans toute la suite de cours, on suppose que $\Omega$ est fini. 
\end{Df}
\begin{Ex}[Lancer de un dé]
Pour l'expérience aléatoire du lancer d'un dé, l'univers est $\Omega=\{1,2,3,4,5,6\}$. 
\end{Ex}


\begin{Df}[Événement]
Un \defi{événement} est une partie de l'univers. Un événement est dit \defi{élémentaire} si il contient une unique issue.
\end{Df} 
\begin{Ex}
Pour l'expérience aléatoire du lancer d'un dé, l'événement A "Obtenir un chiffre pair" est l'ensemble $\{2,4,6\}$. Il n'est pas élémentaire car il est composé de trois issues : 2, 4 et 6.
\end{Ex}

\begin{Df}[Notions et opérations]
Les notions et opérations que l'on définit sur les événements correspondent aux notions et opérations  que l'on définit sur les ensembles.
\begin{center}
\begin{longtable}{|p{1.4cm}|l|p{3cm}|l|}
\hline
Notations&	Vocabulaire ensembliste&	Vocabulaire probabiliste& Diagramme de Venn	\\
\hline
\hline
$\emptyset$&	ensemble vide&	événement impossible&	\\
\hline
$\Omega$&	ensemble plein&	événement certain& \begin{tikzpicture}[scale=0.5]\fill[color=cyan] \E;\draw \E;\end{tikzpicture}	\\
\hline
$\omega$&	élément de $\Omega$& issue&\begin{tikzpicture}[scale=0.5]\draw \E;\node[above right] at (0,0) {$\omega$};\node at (0,0) {$\bullet$};\end{tikzpicture}	\\
\hline
$\{\omega\}$&	singleton&	événement élémentaire&	\\
\hline
$A$&	sous-ensemble de $\Omega$&	événement& \begin{tikzpicture}[scale=0.5]
\draw  \E;
\fill[color=cyan]  \A;
\draw  \A;
\end{tikzpicture}		\\
\hline
$\omega\in A$&	 $\omega$ appartient à $A$&	 Le résultat $\omega$ est une des réalisations possibles de $A$	&	\begin{tikzpicture}[scale=0.5]
\draw  \E;
\draw  \A;
\node[above right] at (-1,0) {$\omega$};\node at (-1,0) {$\bullet$};
\end{tikzpicture}\\
\hline
 $A\subset B$&	 $A$ est inclus dans $B$&	 $A$ implique $B$&\begin{tikzpicture}[scale=0.5]
\draw  \E;
\draw (0.3,0.3) ++(45:2) circle (1);\node at (1.4,0.75) {$A$};
\draw  \B;
\end{tikzpicture}	\\
\hline
 $A\cup B$&	réunion de $A$ et $B$&	 $A$ ou $B$	& \begin{tikzpicture}[scale=0.5]
\draw  \E;
\fill[color=cyan]  \AuB;
\draw  \A;
\draw  \B;
\end{tikzpicture}\\
\hline
 $A\cap B$&	intersection de $A$ et $B$&	 $A$ et $B$&\begin{tikzpicture}[scale=0.5]
\draw  \E;
\draw  \A;
\draw  \B;
\fill[color=cyan]  \AnB;
\end{tikzpicture}
 	\\
\hline
 $\bar A$ (noté aussi $A^{c}$)&	complémentaire de $A$ dans  $\Omega$	&	événement contraire de $A$&\begin{tikzpicture}[scale=0.5]
\fill[color=cyan]  \E;
\fill[color=white]  \A;
\draw  \A;
\node at (2,0) {$\bar A$};
\end{tikzpicture}	\\
\hline
 $ A\cap B=\emptyset$&	 $A$ et $B$ sont disjoints&	$A$ et $B$ sont incompatibles&\begin{tikzpicture}[scale=0.5]
\draw  \E;
\draw  \A;
\draw (1.5,0) ++(45:2) circle (1);\node at (2.4,0) {$B$};
\end{tikzpicture}	\\
\hline
\end{longtable}
\end{center}
\end{Df} 

\begin{Df}[Tribu]
L'ensemble des événements possibles appelé \defi{tribu} est l'ensemble des parties de l'univers, soit $\mathcal{P}(\Omega)$.
\end{Df}
\begin{Ex}
Pour le lancer d'une pièce, l'univers est $\{P,F\}$ et la tribu est $ \{\varnothing, \{P \}, \{F \}, \{P, F \} \}$.
\end{Ex}

\begin{Df}[Espace probabilisable]
Le couple $(\Omega,\mathcal{P}(\Omega))$ est appelé \defi{espace probabilisable}.
\end{Df}

\begin{Df}[Axiomes des probabilités]
Soit $(\Omega,\mathcal{P}(\Omega))$ un espace probabilisable fini.
Une \defi{probabilité} sur $(\Omega,\mathcal{P}(\Omega))$ est une application
de $\mathcal{P}(\Omega)$ dans $\R^+$ vérifiant :
\begin{itemize}
\item
  $P(\Omega)=1$.
\item
  Pour deux événements disjoints $A$ et $B$, on a  :
$$P(A\cup B)=P(A)+P(B)$$  
\end{itemize}
Le triplet  $(\Omega,\mathcal{P}(\Omega),P)$ s'appelle un \defi{espace probabilisé}.
\end{Df}



%TODO faire les diagrammes de VEIN
\begin{Prop}
\begin{enumerate}
\item $ P(\emptyset)=0.$
\item Pour tout $A,B\in \mathcal{A}$ tel que  $A\subset B$, on a :
$$P(A)\leq P(B)$$
$$P(B\setminus A )=P(B)-P(A)$$
\item Pour tout $A\in \mathcal{A}$, on a $P(A)\in[0,1]$.
\item Pour tout $A,B\in \mathcal{A}$, on a $$P(A\cup B)+P(A\cap B)=P(A)+P(B).$$
\item Pour tout $A,B\in \mathcal{A}$, on a $$P(A\cap B)\leq P(A)+P(B).$$
\item Si $A_1, \dots, A_n$ est une famille d'événements deux à deux incompatibles, alors
alors \[ P\left( \cup_{i=1}^n A_i \right) = \sum_{i=1}^n P(A_i) \]
\item \defi{Sous additivité} :Si $A_1, \dots, A_n$ est une famille d'événements , alors  :
\[ P\left( \cup_{i=1}^n A_i \right) \leq  \sum_{i=1}^n P(A_i) \]
\end{enumerate}
\end{Prop}
\begin{proof}
\begin{enumerate}
\item Les deux événements  $\Omega$ et $\emptyset$ sont disjoints d'où :
\begin{align*}
P(\Omega)=&P(\Omega\cup \emptyset )\\
P(\Omega)=&P(\Omega)+P(\emptyset)\\
P(\emptyset)=&0.
\end{align*}
\item Comme $A\subset B$, on a  $A \bigcup\limits_{\text{disjoints}}(B\setminus A)=B$, d'où :
$$ P(A)+P(B\setminus A)=P(B)$$
et 
$$ P(A)\leq P(B)$$
\item Comme $A\subset \Omega$, on a $P(A)\leq P(\Omega)=1$.
\item Comme $A\cup B=(A\setminus (A\cap B))\bigcup\limits_{\text{disjoints}}B$, on a $$P(A\cup B)=P(A\setminus (A\cap B))+P(B).$$ Comme $(A\cap B)\subset A$, on a bien : 
 $$P(A\cup B)=P(A)- P(A\cap B)+P(B).$$
\item Comme $P(A\cup B)+ P(A\cap B)=P(A)+P(B)$, on a $P(A\cup B)\leq P(A)+P(B)$.
\item Démonstration par récurrence :\\
Soit $H_n$ la propriété si $A_1, \dots, A_n$ est une famille d'événements deux à deux incompatibles,
alors \[ P\left( \cup_{i=1}^n A_i \right) = \sum_{i=1}^n P(A_i) \].
\begin{itemize}
\item $H_1$ : on a bien  $P(A)=P(A)$.
\item $H_n\Rightarrow H_{n+1}$ : Soit $A_1, \dots, A_n, A_{n+1}$ une famille d'événements deux à deux incompatibles. On a:
\begin{align*}
P\left( \cup_{i=1}^{n+1} A_i \right)&=P(\left( \cup_{i=1}^{n} A_i \right)\cup A_{n+1} )\\
&=P\left( \cup_{i=1}^{n} A_i \right)+ P (A_{n+1} ) \text{ car } \cup_{i=1}^{n} A_i \text{ et } A_{n+1} \text{ sont disjoints} \\
&= \sum_{i=1}^n P(A_i)+ P (A_{n+1} ) \text{ car } H_n \\
&= \sum_{i=1}^{n+1} P(A_i).
\end{align*}
\end{itemize}
\item Démonstration identique à la précédente en remplaçant les égalités par inégalités.
\end{enumerate}
\end{proof}

\begin{Df}[Probabilité uniforme]
On appelle \defi{probabilité uniforme} sur $\Omega$ la probabilité définie par :
$$P(A)=\frac{card(A)}{card(\Omega)}.$$
\end{Df}

\begin{Ex}[Lancer de deux dés]
Pour l'expérience aléatoire du lancer de deux dés, l'univers est $\Omega=\overbrace{\{1,2,3,4,5,6\}}^{\text{Issues du premier dé}}\times \overbrace{\{1,2,3,4,5,6\}}^{\text{Issues du second dé}}$ et la probabilité est la probabilité uniforme.\\
L'événement A: "Somme des chiffres égale à 2 ou 12" est $\{(1,1),(6,6)\}$.\\
L'événement B: "Somme des chiffres égale à 7" est $\{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\}$. \\
D'où
$$P(A)=\frac{card(A)}{card(\Omega)}=\frac{2}{36}=\frac{1}{18},$$
et
 $$P(B)=\frac{card(B)}{card(\Omega)}=\frac{6}{36}=\frac{1}{6}.$$
\end{Ex}

\begin{Prop}
Une probabilité $P$ sur un univers fini est complètement déterminée par les $P(\{\omega\})$ pour tout $\omega \in \Omega$. $P(\{\omega\})$, appelé poids de probabilité.\\
En effet, pour $A \subset \Omega$, on a :
\begin{align*}
P(A)&=P(\bigcup\limits_{\omega\in A}\{w\} ),\\
 &=\sum_{\omega\in A}P(\{w\}).
\end{align*}
\end{Prop}
\begin{NB}
\begin{itemize}
\item Les poids d'une probabilité $P$ vérifient
$$ \sum_{\omega \in \Omega}P(\{\omega\}) = 1.$$
\item Il est souvent plus facile de définir une probabilité sur les événements élémentaires ("les issues") que sur l'ensemble des événements.\\
Une probabilité sur $\Omega=\{\omega_1,\ldots,\omega_n\}$ est la donnée d'une suite finie $(p_1,\ldots,p_n)$ de nombres tels que :
\begin{enumerate}
\item $p_i = P(\{\omega_i\})$
\item $0\leq p_i\leq 1$
\item $p_1+p_2+\ldots+p_n=1.$
\end{enumerate}
\end{itemize}
\end{NB}
\begin{Ex}
Une dé est pipé tel que la chance d'obtenir le chiffre 6 soit 2 fois plus grande que les autres chiffre. Dans ce cas, on a donc :
$$\begin{cases}
p_6=2 p_1=2 p_2=2 p_3=2 p_4=2 p_5 \\
p_1+p_2+p_3+p_4+p_5+p_6=1
\end{cases}$$
Après résolution, la probabilité est donc définie par
$$p_1=p_2=p_3=p_4=p_5=\frac 1 7 \text{ et }  p_6=\frac 2 7.$$
\end{Ex}



\begin{Df}[système complet d'événements]
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.\\
Un \defi{système complet d'événements}, appelé aussi \defi{partition}, est une famille $(A_1,\ldots,A_n)$
d'événements deux à deux incompatibles tels que
\[ \bigcup\limits_{i=1}^n A_i = \Omega . \]
\begin{center}
\begin{tikzpicture}[scale=0.75]

%\fill[color=cyan!50] (0,1.5) ellipse (2.5 and 2);
\fill[red] (-3,-1) rectangle (-1,2);\node at (-2,1.5) {$A_1$};
\fill[opacity=0.5,green] (-1,-1) rectangle (1,2);\node at (0,1.5) {$A_2$};
\fill[opacity=0.5,yellow] (1,-1) rectangle (3,2);\node at (2,1.5) {$A_3$};
\draw (-3,-1) rectangle (3,2);\node[above right] at (-3,-1) {$\Omega$};
\end{tikzpicture}\\
$A_1,A_2,A_3$ est un système complet d'événements de $\Omega$.
\end{center}
\end{Df}
\begin{NB}[$A,\bar A$]
Pour tout événement $A$, $A,\bar A$ est un système complet d'événements.
\end{NB}
\begin{Prop}[système complet d'événements]
\label{prop:sce}
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.\\
Soit$(A_1,\ldots,A_n)$ un système complet d'événements.\\
On a :
\[ 1 = \sum_{i=1}^n P(A_i). \]
Pour tout événement $B$, on a
\[ P(B) = \sum_{i=1}^n P(B\cap A_i). \]
\begin{center}
\begin{tikzpicture}[scale=0.75]
\draw (-3,-1) rectangle (3,4);\node[above right] at (-3,-1) {$\Omega$};
\fill[color=cyan!80] (0,1.5) ellipse (3 and 1.5);
\fill[opacity=0.5,red] (-3,-1) rectangle (-1,4);\node at (-2,3.5) {$A_1$};
\fill[opacity=0.5,green] (-1,-1) rectangle (1,4);\node at (0,3.5) {$A_2$};
\fill[opacity=0.5,yellow] (1,-1) rectangle (3,4);\node at (2,3.5) {$A_3$};
\node at (0,1.5) {$A_2\cap B$};
\node at (-2,1.5) {$A_1\cap B$};
\node at (2,1.5) {$A_3\cap B$};
\node at (0,-0.4) {$B$};
\end{tikzpicture}
\end{center}

\end{Prop}

\begin{NB}
En particulier, pour tout événement $A$, on a :
$$P(A)+P(\bar A)=1$$
et pour tout événement $B$, :
$$P(B) = P(B\cap A)+P(B\cap\bar A)$$
\end{NB}


\begin{proof}
On a :
\begin{align*}
P(B)&=P(B\cap \Omega )\\
P(B)&=P(B\cap \left(\bigcup\limits_{i=1}^n A_i\right ) )\\
P(B)&=P(\bigcup\limits_{i=1}^n (B\cap A_i))\\
P(B)&=\sum_{i=1}^n P(B\cap A_i).
\end{align*}
En particulier si $B=\Omega$, on a :
\begin{align*}
P(\Omega)&=\sum_{i=1}^n P(\Omega\cap A_i)\\
1&=\sum_{i=1}^n P(A_i)
\end{align*}
\end{proof}
\begin{NB}
Un système complet d'événements intervient dans la modélisation d'une expérience aléatoire à plusieurs étapes comme dans l'exemple ci-dessous.  
\end{NB}
\begin{Ex}[3 urnes]
On dispose de 3 urnes $U_1$, $U_2$, $U_3$, chacune contient 10 boules; parmi elles, $U_1$ contient 1 blanche, $U_2$ contient 2 blanches, et $U_3$ contient 6 blanches. On tire au hasard une urne puis  une boule dans cette urne. Quelle est la probabilité de l'événement B:"obtenir une blanche" ?\\
Les événements $U_1$: "tirer l'urne 1,  $U_2$: "tirer l'urne 2", $U_3$::"tirer l'urne 3" sont un système complet d'événements. On a donc :
$$P(B)= P(B\cap U_1)+P(B\cap U_2)+P(B\cap U_3).$$
Il reste à déterminer les probabilité de cette somme ce qui est l'objet des probabilités conditionnelles.
\end{Ex}

\subsection{Probabilité conditionnelle}
Dans l'expérience aléatoire des trois urnes, la probabilité de tirer une boule blanche est modifié si l'on dispose de l'information l'urne 1 a été choisie.\\   
Le concept de probabilité conditionnelle permet de prendre en compte ce complément d'information.\\
Par exemple, une classe est constituée de $N$ élèves, dont 
\begin{enumerate}
\item $N_h$ hommes,
\item $N_m$ majeurs,
\item $N_{h\cap m}$ hommes et majeurs.
\end{enumerate}
Un élève passant au tableau est choisi aléatoirement. \\
On note $H=$"élève choisi est un homme" et  $M=$"élève choisi est majeur".\\
L'équiprobabilité donne  :
$$ P(M) = \frac{N_m}{N}\text{ et } P(H\cap M)=\frac{N_{h\cap m}}{N}.$$
Quelle est la probabilité que l'élève choisie soit un homme sachant qu'il doit être majeur, noté $P_M(H)$ ? 
Dans cette expérience aléatoire, l'information supplémentaire est l'élève choisi  est majeur. L'univers est maintenant l'ensemble des élèves majeurs donc la probabilité est :
$$P_M(H) =\frac{N_{h\cap m}}{N_m}.$$
Mais,
$$P_M(H) =\frac{N_{h\cap m}}{N_m}=\frac{\frac{N_{h\cap m}}{N}}{\frac{N_m}{N}}=\frac{P(H\cap M)}{P(M)}.$$ 
Par analogie, la définition formelle est :
\begin{Df}[Probabilité conditionnelle]

Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.
Soit $A$ un événement tel que $P(A)>0$.
L'application
\[ \Fonction{P_A}{\mathcal{P}(\Omega)}{[0,1]}{B}{\frac{P(A\cap B)}{P(A)}} \]
est une probabilité sur $(\Omega,\mathcal{P}(\Omega))$
appelée \defi{probabilité conditionnellement à $A$},
ou \defi{probabilité sachant $A$}.\\
On note $P(B|A) = P_A(B)$.
\end{Df}

\begin{Ex}
On lance un dé parfaitement équilibré. La probabilité d'obtenir un 6 est 1/6. On suppose maintenant que ce dé a ses faces impaires peintes en vert, et ses faces paires peintes en bleu. On a aperçu de loin que, sur le dessus du dé, on a obtenu une face bleue. Quelle est la probabilité d'obtenir 6 sachant une face bleue ?\\
On note $B=\{2,4,6\}$ l'événement "face bleue". On a :
$$P_B(\{6\})=\frac{P(\{6\}\cap B)}{P(B)}=\frac{\frac 1 6 }{\frac 1 2 }=\frac 1 3.$$
\end{Ex}


\begin{Prop}[Formule des probabilités totales]
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.\\
Soit $(A_1,\ldots,A_n)$ un système complet d'événements de probabilités non nulles.\\
Pour tout événement $B$, on a
\[ P(B) = \sum_{i=1}^n P_{A_i}(B)P(A_i). \]
\end{Prop}
\begin{proof}
Dans l'équation d'un système complet d'événements  de la proposition~\ref{prop:sce}, il suffit de remplacer $P(B\cap A_i)$ pat $P(B|A_i)P(A_i).$
\end{proof}
\begin{Ex}[3 urnes]
La probabilité de tirer une boule blanche était de :
$$P(B)= P(B\cap U_1)+P(B\cap U_2)+P(B\cap U_3).$$
Ce qui donne :
$$P(B)= P_{U_1}(B)P(U_1)+P_{U_1}(B)P(U_2)+P_{U_3}(B)P(U_3)=\frac{1}{10} \frac 1 3+ \frac{2}{10} \frac 1 3 +\frac{3}{10} \frac 1 3 =\frac{6}{30} =\frac{1}{5} .$$
\end{Ex}
\begin{NB}
En particulier, si  $A$ est un événement de probabilité non nulle, on a pour tout événement $B$:
\[ P(B) = P_A(B)P(A) + P_{\bar A}(B)P(\bar A).\]
\end{NB}



\begin{Prop}[formule de Bayes]
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.\\
Soit $A$ et $B$ deux événements de probabilités non nulles.\\
Alors
$$ P_B(A) = \frac{ P_A(B)P(A) }{ P(B) } $$
\end{Prop}
Cette formule permet d'inverser des conditions. Sa version simple découle de façon directe de la
définition d'une probabilité conditionnelle.
\begin{Prop}[formule de Bayes usuelle]
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.
Soit $(A_1,\ldots,A_n)$ un système complet d'événements de probabilités non nulles.
Pour tout événement $B$ de probabilité non nulle,
et pour tout $k\in\{1,n\}$, on a
\[ P_B(A_k) = \frac{ P_{A_k}(B)P(A_k) }{ \sum_{i=1}^n P_{A_i}(B)P(A_i) }. \]
\end{Prop}

\begin{NB}
Le système $(A_1,\ldots,A_n)$ représente souvent une liste de causes pouvant amener l'événement B lors d'une étape
suivante de l'expérience par exemple dans le problème des 3 urnes. Il est alors généralement facile de déterminer la probabilité qu'une certaine
conséquence $B$ ait lieu, sachant que la cause $A_i$ a eu lieu, c'est-à-dire la probabilité conditionnelle
$P_{A_i}(B)$ en respectant l'ordre temporel. Ces données permettent, grâce à la formule de
Bayes, de remonter le temps, en déterminant la probabilité qu'une certaine cause $A_i$ ait eu lieu sachant
la conséquence $B$. Pour cette raison, cette formule est aussi souvent appelée formule de probabilité des
causes.
\end{NB}
\begin{Ex}[3 urnes]
On cherche à connaître la probabilité que l'urne 1 est été choisie sachant que l'on a tiré une boule blanche, c'est à dire $P_B(U_1)$.\\
On a donc 
$$ P_B(U_1) = \frac{ P_{U_1}(B)P(U_1) }{  P_{U_1}(B)P(U_1)+ P_{U_2}(B)P(U_2)+ P_{U_3}(B)P(U_3) }=\frac{\frac{1}{3}\frac{1}{10}}{\frac{1}{3}\frac{1}{10}+\frac{1}{3}\frac{2}{10}+\frac{1}{3}\frac{6}{10}}=\frac{1}{9}.$$
\end{Ex}


\begin{Ex}[Test de dépistage]
Vous êtes directeur de cabinet du ministre de la santé. Une maladie est présente dans la population, dans la proportion d'une personne malade sur 10000. Un responsable d'un grand laboratoire pharmaceutique vient vous vanter son nouveau test de dépistage : si une personne est malade, le test est positif à 99$\%$. Si une personne n'est pas malade, le test est positif à 0,1$\%$.
  Ces chiffres ont l'air excellent, vous ne pouvez qu'en convenir. Toutefois, avant d'autoriser la commercialisation de ce test, vous faites appel au statisticien du ministère : ce qui vous intéresse, ce n'est pas vraiment les résultats présentés par le laboratoire, c'est la probabilité qu'une personne soit malade si le test est positif. La formule de Bayes permet de calculer cette probabilité.\\
  On note M l'événement : "La personne est malade", et T l'événement : "Le test est positif". Le but est de calculer $P_T(M)$. Les données que vous avez en main sont $P(M)=0,0001$ (et donc $P(\bar M)=0,9999$), $P_M(T)=0,99$ et $P_{\bar M}(T)=0,001$. La formule de Bayes donne :
$$P_T( \bar M)=\frac{P_{\bar M}(T)P(\bar M)}{P_M(T)P(M)+P_{\bar M}(T)P(\bar M)}=\frac{0,9999×10^{-3}}{10^{-4}0,99+0,9999×10^{-3}}\approx 0,91.$$
C'est catastrophique! Il y a que 91$\%$ de chances qu'une personne positive au test ne soit pas malade! C'est tout le problème des tests de dépistage pour des maladies rares : ils doivent être excessivement performants, sous peine de donner beaucoup trop de "faux-positifs".
\end{Ex}


\begin{Df}[arbre de probabilité]
Un \defi{arbre de probabilité} est un schéma permettant de résumer une expérience aléatoire connaissant des probabilités conditionnelles, soit un graphe orienté et pondéré obéissant aux règles suivantes :
\begin{enumerate}
\item la somme des probabilités des branches issues d'un même sommet donne 1,
\item la probabilité d'un chemin est le produit des probabilités des branches qui le composent,
\item la probabilités de la branche allant du sommet A vers le sommet B est la probabilité conditionnelle de $B$ sachant que $A$ est déjà réalisé $P_A(B)$.
\end{enumerate}
On retrouve alors la propriété de la probabilité conditionnelle :
$$ P(A\cap B)= P_{A}(B)P(A).$$
Ainsi que la formule des probabilités totales, si $A_1, A_2, \dots , A_n$ définit un système complet d'évènements de probabilité non nulles, on a :
$$ P(B)=\sum _{i=1}^{n}P(B\cap A_{i})=\sum _{i=1}^{n} P_{A_{i}}(B)P(A_{i}).$$
\end{Df}
%
\begin{Ex}[3 urnes]
%
\begin{tikzpicture}[scale=1.5]
\draw (6.1,3) node[above]{\'Evénements};
\draw  (0,0)  -- (1.8,2) node[midway,sloped,above]{$P(U_1)=\frac 1 3 $};
\draw (2,2) node {$U_1$} ;
\draw  (2.2,2) -- (4.8,1.3)node[midway,sloped,above]{$P_{U_1}(\bar{B})=\frac{9}{10}$};
\draw (4.9,1.3) node[right] {$\bar{B}\quad U_1\cap \bar{B}$} ;    
\draw  (2.2,2) -- (4.8,2.7)node[midway,sloped,above]{$P_{U_1}(B)=\frac{1}{10}$};
\draw (4.9,2.7) node[right] {$B\quad U_1\cap B$} ;

\draw  (0,0) -- (1.8,0)node[midway,sloped,above]{$P(U_2)=\frac 1 3 $};
\draw (2,0) node {$U_2$} ;
\draw  (2.2,0) -- (4.8,-0.7)node[midway,sloped,above]{$P_{U_2}(\bar{B})=\frac{8}{10}$};
\draw (4.9,-0.7) node[right] {$\bar{B}\quad U_2\cap \bar{B}$} ;    
\draw  (2.2,0) -- (4.8,0.7)node[midway,sloped,above]{$P_{U_2}(B)=\frac{2}{10}$};
\draw (4.9,0.7) node[right] {$B\quad U_2\cap B$} ;

\draw  (0,0) -- (1.8,-2)node[midway,sloped,above]{$P(U_3)=\frac 1 3 $};
\draw (2,-2) node {$U_3$} ;
\draw  (2.2,-2) -- (4.8,-2.7)node[midway,sloped,above]{$P_{U_3}(\bar{B})=\frac{4}{10}$};
\draw (4.9,-2.7) node[right] {$\bar{B}\quad U_3\cap \bar{B}$} ;    
\draw  (2.2,-2) -- (4.8,-1.3)node[midway,sloped,above]{$P_{U_3}(B)=\frac{6}{10}$};
\draw (4.9,-1.3) node[right] {$B\quad  U_3\cap B$} ;
\end{tikzpicture}\\
La probabilité de  l'événement $B$ est la somme des probabilités des parcours
qui mènent à B, d'où 
$$P(B)= P_{U_1}(B)P(U_1)+P_{U_1}(B)P(U_2)+P_{U_1}(B)P(U_3).$$
\end{Ex}

% -----------------------------------------------------------------------------
\subsection{Indépendance}
On considère l'expérience aléatoire de lancer successivement deux fois un dé.\\
Soit  $A$ un événement relié au premier lancer, par exemple obtenir un chiffre pair sur le premier dé et 
soit  $B$ un événement relié au second lancer, par exemple obtenir un chiffre impair sur le second dé.
L'information de savoir que l'événement $A$ est réalisé, ne modifie pas la probabilité de l'événement $B$, autrement dit $P_A(B) = P(B)$. 
Mais, d'après la définition de la probabilité conditionnelle, on a : $P_A(B) =\frac{P(A\cap B)}{P(A)}$. De ces deux égalités, on obtient $P(A\cap B)=P(A)P(B)$ et on dit que les deux événements sont indépendants.\\ 
Par analogie, la définition formelle est :
\begin{Df}[indépendance]

Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.
\begin{itemize}
\item
  Deux événements $A$ et $B$ sont \defi{indépendants} si $P(A\cap B)=P(A)P(B)$.
\item
  Une famille $(A_1,\ldots,A_n)$ d'événements sont \defi{(mutuellement) indépendants}
  si pour toute partie $I\subset\{1,\ldots,n\}$, on a
  \[ P\left( \bigcap_{i\in I} A_i \right) = \prod_{i\in I} P(A_i).\]
\end{itemize}
\end{Df}
\begin{Ex}[Lancer de deux dés]
$A$="Obtenir 5 ou 6 sur le premier dé"\\
$A=\{(5,1),(5,2),(5,3),(5,4),(5,5),(5,6),(6,1),(6,2),(6,3),(6,4),(6,5),(6,6)\}$.\\
$B$="Obtenir  6 sur le second dé"\\
$B=\{(1,6),(2,6),(3,6),(4,6),(5,6),(6,6)\}$\\
$A\cap B=$"Obtenir 5 ou 6 sur le premier dé et obtenir  6 sur le second dé".\\
$A\cap B= \{ (5,6),(5,6),(6,6)\}.$
On a $P(A)=\frac{12}{36}=\frac 1 3 , P(B)=\frac{6}{36}=\frac 1 6$ et $P(A\cap B)=\frac{3}{36}=\frac{1}{12}= P(A)P(B)$.\\
Donc les événements $A$ et $B$ sont indépendants.
\end{Ex}

\begin{NB}
Trois événements peuvent être indépendants deux à deux, sans pour autant être mutuellement indépendants. 
Par exemple, on considère un dé équilibré à 4 faces ($\Omega=\{1,2,3,4\}$ avec $p_1=p_2=p_3=p_4=\frac 1 4$ et les événements :
$$A=\{1,2\},\quad B=\{1=3\},\quad C=\{1,4\}.$$
On a : $$P(A)=P(B)=P(C)=\frac 1 2$$ et $$P(A\cap B)=(A\cap C)=P(B\cap C)=P(\{1\})=\frac 1 4 =P(A)P(B)=P(A)P(C)=P(B)P(C).$$
Les événements $A$, $B$ et $C$ sont deux à deux indépendants mais :
 $$P(A\cap B\cap C )= P(\{1\})=\frac 1 4 \neq  \frac 1 8 =  P(A)P(B)P(C).$$
 Donc les événements ne sont pas mutuellement indépendants.
\end{NB}
\begin{Prop}[lien avec la probabilité conditionnelle]
Soit $(\Omega,\mathcal{P}(\Omega),P)$ un espace probabilisé.\\
Soit $A$ un événement de probabilité non nulle
et $B$ un événement quelconque.\\
Alors les événements $A$ et $B$ sont indépendants si et seulement si $P_A(B)=P(B)$.
\end{Prop}
\begin{proof}
Soit $A$ et $B$ deux événements tel que $P(A)\geq 0$.\\
\begin{align*}
A, B \text{ sont indépendants } & \Leftrightarrow P(B\cap A)=P(B)P(A)\\
& \Leftrightarrow \frac{P(B\cap A)}{P(A)}=P(B) \text{ car }P(A)\neq 0\\
& \Leftrightarrow P_A(B)=P(B).
\end{align*} 
\end{proof}

\begin{Prop}[Formule des probabilités composées]
Soit $A_1,A_2,\dots, A_n$ $n$ événements vérifiant $P(A_1\cap A_2\cap \dots \cap A_{n-1})\neq 0$.\\
Alors 
$$P(A_1\cap A_2\cap \dots \cap A_{n})=P(A_1)P_{A_1}(A_2)P_{A_1\cap A_2 }(A_3)\dots P_{A_1\cap A_2\cap \dots \cap A_{n-1}}(A_n).$$
\end{Prop}
\begin{tikzpicture}[scale=1.5]
\node (start) at (0,0) {$\Omega$};
\node (A1) at (1.5,1) {$A_1$};
\node (A1f) at (1.5,0) {};
\node (A2) at (3,2) {$A_2$};
\node (A2f) at (3,1) {};
\node (A3) at (4.5,3) {};
\node (A3f) at (4.5,2) {};
\node (A4) at (7.2,3) {$A_3:\quad P(A_1\cap A_2\cap A_{3})=P(A_1)P_{A_1}(A_2)P_{A_1\cap A_2 }(A_3)$};
\draw(start) -- (A1)node[midway,sloped,above]{$P(A_1)$};
\draw[dashed] (start) -- (A1f);
\draw(A1) -- (A2)node[midway,sloped,above]{$P_{A_1}(A_2)$};
\draw[dashed] (A1) -- (A2f);
\draw(A2) -- (A3)node[midway,sloped,above]{$P_{A_3}(A_1\cap A_2)$};
\draw[dashed] (A2) -- (A3f);
\end{tikzpicture}\\
\begin{proof}
Il suffit de remonter le temps : 
$$P(A_1\cap A_2\cap \dots \cap A_{n})=P(A_1\cap A_2\cap \dots \cap A_{n-1}) P_{A_1\cap A_2\cap \dots \cap A_{n-1}}(A_n).$$
Puis :
$$P(A_1\cap A_2\cap \dots \cap A_{n})=P(A_1\cap A_2\cap \dots \cap A_{n-2}) P_{A_1\cap A_2\cap \dots \cap A_{n-2}}(A_{n-1})P_{A_1\cap A_2\cap \dots \cap A_{n-1}}(A_n).$$
On itère jusqu'à $A_1$.
\end{proof}


%TODO voir cours de mpsi sur les méthodes


